[
  {
    "objectID": "notes/selectivity.html",
    "href": "notes/selectivity.html",
    "title": "Selectivity",
    "section": "",
    "text": "Selectivity: relative vulnerability of a demographic group of the fished population to capture by a fishery or survey, with at least one demographic group being fully selected (Cadrin et al., 2015).\nSelectivity is a combination of two processes:\n\nthe relative probability of capture for a demographic group (i.e., contact selectivity; Miller and Fryer, 1999)\nproportion of the group that is available to the fishery in time and space (i.e., population selectivity; Miller and Fryer, 1999)\n\naka vulnerability\n\n\nAssessments models typically require a particular form of selection curve and estimates a population selectivity curve. Selectivity is used to link observed composition data to model predictions about population abundance-at-age/-size. If multiple gear types operate in the fishery, or if the catch-at-age compositions from different segments of the fleet have distinct characteristics, then the catch-at-age data can be partitioned into separate matrices for each gear or fleet type with separate selectivity curves and parameters for each type.\n\n\nSelectivity is the portion of a demographic group that is vulnerable to capture by fishing:\n\\[\nF_{t,a} = F_t s_a\n\\tag{1}\\]\n\n\\(F_t\\) is fishing mortality at time \\(t\\) for fully-vulnerable ages (i.e., a year-specific fishing mortality multiplier)\n\ntypically managed through limiting total fishing effort in a year\n\n\\(s_a\\) is the selectivity of the age group\n\ntypically managed by regulating fishing technology and behavior\n\n\nThis definition is typically used for age-structure population models, but the same equation can be related to any other demographic groups (e.g., size intervals, life history stages). Eq 1 quantifies either constant or average selectivity during the time interval. The function expressed in eq 1 is termed separability because \\(F_{t,a}\\) can be separated into the components \\(F_t\\) and \\(s_a\\).\nRecall that catchability (\\(q\\)) is the effect of of a unit of fishing effort (\\(E\\)) directed on the population, with the effect measured as the exponential rate of fishing mortality imposed on the population over a time interval \\(t\\):\n\\[\nF_t = q E_t\n\\tag{2}\\]\nConsidering that catchability and selectivity have a relationship with fishing mortality, Eq 1 & 2 can be combined:\n\\[\nF_{t,a} = s_a q E_t; \\hspace{4mm} q_a = s_a  q\n\\tag{3}\\]\nSelectivity then plays an essential role as a component of the survival equation in abundance at age:\n\\[\nN_{t,a} = N_{t-1,a-1}e^{-(F_{t-1} s_{a-1} + M_{t-1,a-1})}\n\\tag{4}\\]\n\n\\(N_{t,a}\\) is the abundance of survivors of an age class at the end of year \\(t\\)\n\nEq 4 is the primary process equation for all age-structured stock assessment models."
  },
  {
    "objectID": "notes/selectivity.html#definition-of-selectivity",
    "href": "notes/selectivity.html#definition-of-selectivity",
    "title": "Selectivity",
    "section": "",
    "text": "Selectivity: relative vulnerability of a demographic group of the fished population to capture by a fishery or survey, with at least one demographic group being fully selected (Cadrin et al., 2015).\nSelectivity is a combination of two processes:\n\nthe relative probability of capture for a demographic group (i.e., contact selectivity; Miller and Fryer, 1999)\nproportion of the group that is available to the fishery in time and space (i.e., population selectivity; Miller and Fryer, 1999)\n\naka vulnerability\n\n\nAssessments models typically require a particular form of selection curve and estimates a population selectivity curve. Selectivity is used to link observed composition data to model predictions about population abundance-at-age/-size. If multiple gear types operate in the fishery, or if the catch-at-age compositions from different segments of the fleet have distinct characteristics, then the catch-at-age data can be partitioned into separate matrices for each gear or fleet type with separate selectivity curves and parameters for each type.\n\n\nSelectivity is the portion of a demographic group that is vulnerable to capture by fishing:\n\\[\nF_{t,a} = F_t s_a\n\\tag{1}\\]\n\n\\(F_t\\) is fishing mortality at time \\(t\\) for fully-vulnerable ages (i.e., a year-specific fishing mortality multiplier)\n\ntypically managed through limiting total fishing effort in a year\n\n\\(s_a\\) is the selectivity of the age group\n\ntypically managed by regulating fishing technology and behavior\n\n\nThis definition is typically used for age-structure population models, but the same equation can be related to any other demographic groups (e.g., size intervals, life history stages). Eq 1 quantifies either constant or average selectivity during the time interval. The function expressed in eq 1 is termed separability because \\(F_{t,a}\\) can be separated into the components \\(F_t\\) and \\(s_a\\).\nRecall that catchability (\\(q\\)) is the effect of of a unit of fishing effort (\\(E\\)) directed on the population, with the effect measured as the exponential rate of fishing mortality imposed on the population over a time interval \\(t\\):\n\\[\nF_t = q E_t\n\\tag{2}\\]\nConsidering that catchability and selectivity have a relationship with fishing mortality, Eq 1 & 2 can be combined:\n\\[\nF_{t,a} = s_a q E_t; \\hspace{4mm} q_a = s_a  q\n\\tag{3}\\]\nSelectivity then plays an essential role as a component of the survival equation in abundance at age:\n\\[\nN_{t,a} = N_{t-1,a-1}e^{-(F_{t-1} s_{a-1} + M_{t-1,a-1})}\n\\tag{4}\\]\n\n\\(N_{t,a}\\) is the abundance of survivors of an age class at the end of year \\(t\\)\n\nEq 4 is the primary process equation for all age-structured stock assessment models."
  },
  {
    "objectID": "notes/selectivity.html#selectivity-functions",
    "href": "notes/selectivity.html#selectivity-functions",
    "title": "Selectivity",
    "section": "2 Selectivity functions",
    "text": "2 Selectivity functions\nThere are several forms of fishing selectivity. Sampson and Scott (2012) defined four categories:\n\nAsymptotic: the oldest age classes are fully vulnerable 1a. Knife-edged: the simplest form of asymptotic form\nIncreasing: selection increases with age\nSaddle: there is at least one local minimum selection in the intermediate range of age classes\nDomed: the age for maximum selectivity (\\(s = 1\\)) is imtermediate in the range of age classes\n\n\n\n2.1 Logistic\nAsymptotic selectivity is often modeled using a logistic function. There are many variations of the logistic selectivity function.\n\\[\ns_a = \\dfrac{1}{1 + exp(-\\sigma_s (a - a_{50}))}\n\\]\n\n\\(\\sigma_s\\) is the logistic slope parameter\n\\(a_{50}\\) is the age of 50% vulnerability to the fishery (i.e., \\(s_{a_{50}} = 0.5\\))\n\nAnother parameterization of logistic selectvity uses parameters \\(a_{50}\\) and \\(a_{95}\\), which represent the ages where a fish has a probability 50% or 95% chance of being captured.\n\\[\ns_a = \\dfrac{1}{1+exp\\left( -log(19)\\left(\\dfrac{a - a_{50}}{a_{95} - a_{50}}\\right) \\right)}\n\\]\n\n\n\n\n\n\nWhy log(19)?\n\n\n\n\n\nWe need to ensure that the logistic function equals 0.95 when \\(a = a_{95}\\) and equals 0.5 when \\(a = a_{50}\\). Think about the odds of being captured:\n\nThe exponent of the log odds equals the odds ratio\nThe odds ratio is the probability of an event (i.e., getting captured) happening over the probablity of it not happening\n\nThus, the log-odds ratio for the age at which we have a 95% probability of getting captured is \\(log(\\dfrac{0.95}{0.05})\\). This can be rewritten as \\(log(\\dfrac{19/20}{1/20})\\), which then simplifies to \\(log(19)\\).\nThis is not necesssary for \\(a_{50}\\) because the odds ratio of two events with 50% probability is equal to zero.\n\n\n\n\n\n2.2 Double logistic\nThe six parameter version of the double normal selectivity function can capture both asymptotic and dome-shaped shapes (commonly used in Stock Synthesis; Methot and Wetzel, 2013). The double normal has three components connected by steep logistic “joiners” to provide overall differentiability:\n\\[\ns_a = {asc}_a (1-j_{1,a}) + j_{1,a}((1-j_{2,a}) + j_{2,a} {dsc}_a)\n\\]\n\n\\({asc}_a\\) is the ascending limb\n\n\\[\n{asc}_a = p_5 + (1-p_5)\\left(e^{-(a - p_1)^2 / e^{p_3}} - e^{{p_1}^2 / e^{p_3}} \\right) / \\left(1 - e^{{p_1}^2 / e^{p_3}} \\right)\n\\]\n\n\\({dsc}_a\\) is the descending limb\n\n\\[\n{dsc}_a = 1 + (p_6 - 1)\\left(e^{-(a - \\gamma)^2 / e^{p_4}} - 1 \\right) / \\left(e^{-(A-\\gamma)^2 / e^{p_4}} - 1 \\right)\n\\]\n\n\\(j_{1,a}\\) is the first joiner function\n\n\\[\nj_{1,a} = \\dfrac{1}{\\left(1+e^{-20(a-p_1)/(1+|a-p_1|)}\\right)}\n\\]\n\n\\(j_{2,a}\\) is the second joiner function\n\n\\[\nj_{2,a} = \\dfrac{1}{\\left(1+e^{-20(a-\\gamma)/(1+|a-\\gamma|)}\\right)}\n\\]\n\n\\(p_1\\) is the age at which selectivity=1 starts\n\\(\\gamma\\) is the age at which selecitivity=1 ends and is defined as:\n\n\\[\n\\gamma = p_1 + 1 + \\left ( \\dfrac{0.99 A - p_1 - 1}{1 + e^{-p_2}} \\right)\n\\]\n\n\\(p_2\\) determines the age at which selectivity=1 ends (the width of the “top”; \\(\\gamma\\) is the end point)\n\\(p_3\\) determines the slope of the ascending section\n\\(p_4\\) determines the slope of the descending section\n\\(p_5\\) is the selectivity at age 0 (in logit-space)\n\\(p_6\\) is the selectivity at age \\(A\\) (in logit-space)\n\nThe double normal can mimic the asymptotic selectivity function by setting \\(p_6 = 1\\).\n\n\n2.3 Gamma\nDome-shaped selectivity can be represented using a gamma function: \\[\ns_a = \\dfrac{a}{a_{max}}^{a_{max}/p} e^{(a_{max}-a)/p}; \\hspace{6mm} p = 0.5 \\left[\\sqrt{{a_{max}}^2 + 4\\gamma^2}- a_{max}\\right]\n\\]\n\n\\(a_{max}\\) is the full vulnerability to the fishery (i.e., \\(s_{a_{max}}\\) = 1)\n\\(\\gamma\\) is the gamma slope parameter"
  },
  {
    "objectID": "notes/selectivity.html#sensitivities-of-selectivity",
    "href": "notes/selectivity.html#sensitivities-of-selectivity",
    "title": "Selectivity",
    "section": "3 Sensitivities of selectivity",
    "text": "3 Sensitivities of selectivity\nThe form of selectivity is assumed in various stock assessment models. \\(F_{t,a}\\) can either be directly estimated or derived from estimated paramters.\n\nCatch curve\n\nEstimates total mortatliy as the negative of log-linear slope in eq 4 by assuming full selectiivty for a range of ages, usually from a middle age to oldest age (i.e., knife-edge selectivity)\nChapman and Robson, 1960\n\nVirtual population analysis (VPA)\n\nAssumes the selectivity of oldest age relative to a younger age to estimate \\(N_{t,a}\\) to derive \\(F_{t,a}\\)\nSelectivity in the most recent year is similar to previous years\nAge range of full vulnerability is often assumed to derive \\(F_t\\) (i.e., knife-edge selectivity)\nShepherd and Pope, 2002\n\nStatistical catch at age (SCAA)\n\nSeparability (eq 1) - estimate a time series of \\(F_t\\) and selectivity (\\(s_a\\)), either as estimated parameters or derived from functional selectivity functions\nMaunder and Punt, 2013"
  },
  {
    "objectID": "notes/selectivity.html#references",
    "href": "notes/selectivity.html#references",
    "title": "Selectivity",
    "section": "4 References",
    "text": "4 References\nCadrin, S.X., DeCelles, G.R. and Reid, D. (2016). Informing fishery assessment and management with field observations of selectivity and efficiency. Fisheries Research, 184, 9-17.\nChapman, D., & Robson, D. S. (1960). The analysis of a catch curve. Biometrics, 354-368.\nMaunder, M. N., & Punt, A. E. (2013). A review of integrated analysis in fisheries stock assessment. Fisheries research, 142, 61-74.\nMethot Jr, R. D., & Wetzel, C. R. (2013). Stock synthesis: a biological and statistical framework for fish stock assessment and fishery management. Fisheries Research, 142, 86-99.\nMillar, R. B., & Fryer, R. J. (1999). Estimating the size-selection curves of towed gears, traps, nets and hooks. Reviews in Fish Biology and Fisheries, 9, 89-116.\nSampson, D. B., & Scott, R. D. (2012). An exploration of the shapes and stability of population–selection curves. Fish and Fisheries, 13(1), 89-104.\nShepherd, J. G., & Pope, J. G. (2002). Dynamic pool models I: Interpreting the past using Virtual Population Analysis. Handbook of Fish Biology and Fisheries: Fisheries, 2, 127-163."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tutorials",
    "section": "",
    "text": "homepage",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "guides/rtmb_intro.html",
    "href": "guides/rtmb_intro.html",
    "title": "RTMB Introduction",
    "section": "",
    "text": "RTMB is a new package that provides a native R interface for a subset of TMB so you can avoid coding in C++\nDeveloped by Kasper Kristensen (DTU-Aqua)\n\nSee https://kaskr.r-universe.dev/RTMB\n\nBecause code is all in R, both easier to code and for others to read that code - a game changer!\nA game changer if you know how to code in R, create an objective f(x) for your model\nNo compiling or compiling errors! (.dll files)\nBottom line: less time developing and testing models, more intuitive code"
  },
  {
    "objectID": "guides/rtmb_intro.html#what-is-rtmb",
    "href": "guides/rtmb_intro.html#what-is-rtmb",
    "title": "RTMB Introduction",
    "section": "",
    "text": "RTMB is a new package that provides a native R interface for a subset of TMB so you can avoid coding in C++\nDeveloped by Kasper Kristensen (DTU-Aqua)\n\nSee https://kaskr.r-universe.dev/RTMB\n\nBecause code is all in R, both easier to code and for others to read that code - a game changer!\nA game changer if you know how to code in R, create an objective f(x) for your model\nNo compiling or compiling errors! (.dll files)\nBottom line: less time developing and testing models, more intuitive code"
  },
  {
    "objectID": "guides/intro_opt.html",
    "href": "guides/intro_opt.html",
    "title": "Introduction to Automatic Differentiation",
    "section": "",
    "text": "The uncertainties of the model and parameters imply that there will always exist a discrepancy between our observations and predictions produced by the model."
  },
  {
    "objectID": "guides/intro_opt.html#mathematical-and-statistical-models",
    "href": "guides/intro_opt.html#mathematical-and-statistical-models",
    "title": "Introduction to Automatic Differentiation",
    "section": "1 Mathematical and statistical models",
    "text": "1 Mathematical and statistical models\nThe objective function is used to define some value that represents the discrepancy between observations and their corresponding model values. In statistics, this value is usually expressed in terms of probability, and referred to as the likelihood value. The optimizer uses a set of computational/mathematical/statistical rules and procedures (or algorithms) to find the best parameter set so that the objective function value is almost zero."
  },
  {
    "objectID": "guides/ADMB_likelihood.html",
    "href": "guides/ADMB_likelihood.html",
    "title": "Likelihood profiling in ADMB using R",
    "section": "",
    "text": "This tutorial will walk through how to conduct a jitter test in ADMB. The fastest way to run a jitter test in ADMB is to run it through the R environment, which will create an executable and run ADMB with the executable so you do not need to manually change the initial parameter values each iteration. However, this will require some extra steps to set up your code in both ADMB and R."
  },
  {
    "objectID": "guides/ADMB_likelihood.html#admb2r.cpp-instructions",
    "href": "guides/ADMB_likelihood.html#admb2r.cpp-instructions",
    "title": "Likelihood profiling in ADMB using R",
    "section": "1 admb2r.cpp instructions",
    "text": "1 admb2r.cpp instructions\nThis tutorial requires this package called “admb2r”. This is a collection of AD Model Builder routines for saving complex data structures into a file that can be read into R. This cannot be automatically downloaded in the newer ADMB versions. You can keep a “admb2r.cpp” file where your .tpl and .dat files are, however that requires copying and pasting it every time you want to run a ADMB model.\n \nThese are instructions for using admb2r.cpp permanently. Copy admb2r.cpp to both the following folders:\n\nadmb/include\nadmb/include/contrib\n\n \nA copy of admb2r.cpp is here. Once this is done you will not need to add admb2r.cpp to every project folder. This has been tested on Linux Mint, Windows, and Mac."
  },
  {
    "objectID": "guides/ADMB_likelihood.html#set-up-admb",
    "href": "guides/ADMB_likelihood.html#set-up-admb",
    "title": "Likelihood profiling in ADMB using R",
    "section": "2 Set up ADMB",
    "text": "2 Set up ADMB\nIf possible, you should download the most recent ADMB version here. This recent version will print out additional error messages and allow you to run additional functions (like get_hessian()).\n\n2.1 Surplus production model - .tpl and .dat files\nIn this tutorial, we will look at a surplus production model. The .dat file name is called surp_prod1.dat. You can name the .tpl file anything, but in this tutorial, it will be called surp_prod_jitter.tpl.\n\nDATA_SECTION\n\n!! ad_comm::change_datafile_name(\"surp_prod1.dat\");\n init_int fyear;\n init_int lyear;\n init_vector cat(fyear,lyear);\n init_vector cpue(fyear,lyear);\n\n\nPARAMETER_SECTION\n\n init_number log_r;\n init_number log_q;\n init_number log_K;\n init_number log_sd_cpue;\n init_vector log_F(fyear,lyear);\n \n number r;\n number q;\n number K;\n number sd_cat;\n number sd_cpue;\n\n vector bio(fyear,lyear+1);\n vector cat_hat(fyear,lyear);\n vector cpue_hat(fyear,lyear);\n vector expl_out(fyear,lyear);\n vector F(fyear,lyear);\n \n objective_function_value jnll;\n\n\nINITIALIZATION_SECTION\n\n log_r -0.6\n log_q -1\n log_K 8.5\n log_sd_cpue -1\n log_F 1\n\n\nPROCEDURE_SECTION\n\n int t;\n dvariable expl;\n \n // Convert from log to normal space\n r = mfexp(log_r);\n q = mfexp(log_q);\n K = mfexp(log_K);\n F = mfexp(log_F);\n sd_cat = 0.05;\n sd_cpue = mfexp(log_sd_cpue);\n\n // Project the model forward\n bio(fyear) = K;\n for (t=fyear; t&lt;=lyear; t++) {\n   expl = 1.0/(1.0+F(t));\n   bio(t+1) = bio(t) + r*bio(t)*(1.0-bio(t)/K) - expl*bio(t);\n   cat_hat(t) = expl * bio(t);\n   expl_out(t) = expl;\n   cpue_hat(t) = q * bio(t);\n  } \n  \n // Compute the likelihoods  \n jnll = 0;\n for (t=fyear; t&lt;=lyear; t++) {\n  jnll += 0.5 * square(log(cat(t)/cat_hat(t)) / sd_cat) + log(sd_cat);\n  jnll += 0.5 * square(log(cpue(t)/cpue_hat(t)) / sd_cpue) + log(sd_cpue);\n }\n\n \nGLOBALS_SECTION\n\n  #include &lt;admodel.h&gt;\n  #include &lt;admb2r.cpp&gt;\n\n\nREPORT_SECTION\n open_r_file(\"out.rdat\", 6, -999);\n  wrt_r_complete_vector(\"obs_cat\", cat);\n  wrt_r_complete_vector(\"obs_cpue\", cpue);\n  wrt_r_complete_vector(\"est_bio\", bio);\n  wrt_r_complete_vector(\"est_cat\", cat_hat);\n  wrt_r_complete_vector(\"est_cpue\", cpue_hat);\n  wrt_r_complete_vector(\"est_expl\", expl_out);\n  wrt_r_item(\"jnll\", jnll);\n close_r_file();\n\n\nFINAL_SECTION\n\n  // extract Hessian matrix\n  open_r_file(\"hessian.rdat\");\n    open_r_matrix(\"hessian\");\n      wrt_r_matrix(get_hessian(),1,1);\n    close_r_matrix();\n  close_r_file();\n\nThis tutorial will not go through the surplus production model in details, but the leading parameters are \\(r\\), \\(K\\), \\(q\\), \\(F\\), and \\(sd_{cpue}\\). Note that there is a standard deviation for the catch observations, but that is fixed in this model (\\(sd_{catch}\\) = 0.05) The objective function (jnll) is the total objective function, which is the sum of the data likelihood for catch and index (CPUE) data, which follow a log normal distribution.\n \nThe default data file name is different than the .tpl file name. This is done using this command:\n\n!! ad_com::change_datafile_name(\"surp_prod1.dat\");\n\nThe data will be read from surp_prod1.dat. This is useful when you have many variants of the model that use the same data. This trick will be useful for the jitter test.\n \nYou can copy and paste the surp_prod1.dat here:\n\n# first year\n1965\n\n# last year\n1988\n\n# Catch\n93.51\n212.444\n195.032\n382.712\n320.43\n402.467\n365.557\n606.084\n377.642\n318.836\n309.374\n389.02\n276.901\n254.251\n170.006\n97.181\n90.523\n176.532\n216.181\n228.672\n212.177\n231.179\n136.942\n212\n\n# Index\n1.78\n1.31\n0.91\n0.96\n0.88\n0.9\n0.87\n0.72\n0.57\n0.45\n0.42\n0.42\n0.49\n0.43\n0.4\n0.45\n0.55\n0.53\n0.58\n0.64\n0.66\n0.65\n0.61\n0.63\n\n\n\n2.2 DATA_SECTION\nIn the surplus production example, we will jitter the following parameters: \\(r\\), \\(K\\), \\(q\\), and \\(sd_{cpue}\\).\n\n!! ad_comm::change_datafile_name(\"log_r.dat\");\n  init_number inlog_r;\n\n!! ad_comm::change_datafile_name(\"log_K.dat\");\n  init_number inlog_K;\n\n!! ad_comm::change_datafile_name(\"log_q.dat\");\n  init_number inlog_q;\n  \n!! ad_comm::change_datafile_name(\"log_sd_cpue.dat\");\n  init_number inlog_sd_cpue;\n\nThis is similar to the previous command, but here the !! ad_com::change_datafile_name(\"param_name.dat\") command is reading parameter values (param_name) from separate .dat files. This will be important in the R script as you can rewrite this .dat file in R to produce multiple starting values that will be read in ADMB. Note that the parameter name that is being read in should be different than the one in the PARAMETER_SECTION (the ones being used in the estimation).\n \nThe DATA_SECTION will look like this:\n\nDATA_SECTION\n!! ad_comm::change_datafile_name(\"log_r.dat\");\n  init_number inlog_r;\n\n!! ad_comm::change_datafile_name(\"log_K.dat\");\n  init_number inlog_K;\n\n!! ad_comm::change_datafile_name(\"log_q.dat\");\n  init_number inlog_q;\n  \n!! ad_comm::change_datafile_name(\"log_sd_cpue.dat\");\n  init_number inlog_sd_cpue;\n\n!! ad_comm::change_datafile_name(\"surp_prod1.dat\");\n init_int fyear;\n init_int lyear;\n init_vector cat(fyear,lyear);\n init_vector cpue(fyear,lyear);\n\n\n\n2.3 PARAMETER_SECTION\nIn this section, we will be overriding the starting parameter values declared in the INITIALIZATION_SECTION. This is why the parameter names above (inparam_name) needs to be different than the ones (param_name) being declared.\n\n!! log_r = inlog_r;\n!! log_K = inlog_K;\n!! log_q = inlog_q;\n!! log_sd_cpue = inlog_sd_cpue;\n\nThis will now read what was stored in the respective .dat files as the starting value of the parameter, which will be created in R.\n \nFor the surplus production model, it will look like this:\n\nPARAMETER_SECTION\n\n init_number log_r;\n init_number log_q;\n init_number log_K;\n init_number log_sd_cpue;\n init_vector log_F(fyear,lyear);\n \n number r;\n number q;\n number K;\n number sd_cat;\n number sd_cpue;\n\n vector bio(fyear,lyear+1);\n vector cat_hat(fyear,lyear);\n vector cpue_hat(fyear,lyear);\n vector expl_out(fyear,lyear);\n vector F(fyear,lyear);\n \n objective_function_value jnll;\n \n // override starting values and read from .dat files\n!! log_r = inlog_r;\n!! log_K = inlog_K;\n!! log_q = inlog_q;\n!! log_sd_cpue = inlog_sd_cpue;\n\n\n\n2.4 INITIALIZATION_SECTION\nThis section should either be commented out or empty as the param_name.dat file will override the starting value for that parameter. This is important for the jitter test to work. It will look like this for the surplus production model:\n\nINITIALIZATION_SECTION\n\n // log_r -0.6\n // log_q -9\n // log_K 8.5\n // log_sd_cpue -1\n log_F 1\n\nThe next few sections (PROCEDURE_SECTION, GLOBALS_SECTION, and REPORT_SECTION) should be the same as the original model.\n\n\n2.5 FINAL_SECTION\nIn the final section, you will need to add a command to define an output file stream, write the output, and close the output file. This is using the functionality of “admb2r”.\n\n  ofstream myout(\"estpars.dat\",ios::app);\n    myout&lt;&lt; inlog_r &lt;&lt; \" \" &lt;&lt; log_r &lt;&lt; \" \" &lt;&lt; inlog_q &lt;&lt; \" \" &lt;&lt; log_q &lt;&lt; \" \" &lt;&lt; inlog_K &lt;&lt; \" \" &lt;&lt; log_K &lt;&lt; \" \" &lt;&lt; inlog_sd_cpue &lt;&lt; \" \" &lt;&lt; log_sd_cpue &lt;&lt; \" \" &lt;&lt; jnll &lt;&lt; endl;\n  myout.close();\n\nThe estpars.dat file will eventually contain all the input starting values from R (inparam_name), estimated parameters from ADMB (param_name), and the objective function (objective_function) from ADMB. This is important as it will contain all the iterations and results of the jitter test and will be read into R as a table.\n \nThe FINAL_SECTION will look like this:\n\nFINAL_SECTION\n\n  ofstream myout(\"estpars.dat\",ios::app);\n    myout&lt;&lt; inlog_r &lt;&lt; \" \" &lt;&lt; log_r &lt;&lt; \" \" &lt;&lt; inlog_q &lt;&lt; \" \" &lt;&lt; log_q &lt;&lt; \" \" &lt;&lt; inlog_K &lt;&lt; \" \" &lt;&lt; log_K &lt;&lt; \" \" &lt;&lt; inlog_sd_cpue &lt;&lt; \" \" &lt;&lt; log_sd_cpue &lt;&lt; \" \" &lt;&lt; jnll &lt;&lt; endl;\n  myout.close();\n\n  // extract Hessian matrix\n  open_r_file(\"hessian.rdat\");\n    open_r_matrix(\"hessian\");\n      wrt_r_matrix(get_hessian(),1,1);\n    close_r_matrix();\n  close_r_file();\n\n\n\n2.6 Entire ADMB example of the surplus production model\nHere is the entire ADMB script for an example of conducting a jitter test with the surplus production model.\n\nDATA_SECTION\n\n!! ad_comm::change_datafile_name(\"log_r.dat\");\n  init_number inlog_r;\n\n!! ad_comm::change_datafile_name(\"log_K.dat\");\n  init_number inlog_K;\n\n!! ad_comm::change_datafile_name(\"log_q.dat\");\n  init_number inlog_q;\n  \n!! ad_comm::change_datafile_name(\"log_sd_cpue.dat\");\n  init_number inlog_sd_cpue;\n\n!! ad_comm::change_datafile_name(\"surp_prod1.dat\");\n init_int fyear;\n init_int lyear;\n init_vector cat(fyear,lyear);\n init_vector cpue(fyear,lyear);\n\n\nPARAMETER_SECTION\n\n init_number log_r;\n init_number log_q;\n init_number log_K;\n init_number log_sd_cpue;\n init_vector log_F(fyear,lyear);\n \n number r;\n number q;\n number K;\n number sd_cat;\n number sd_cpue;\n\n vector bio(fyear,lyear+1);\n vector cat_hat(fyear,lyear);\n vector cpue_hat(fyear,lyear);\n vector expl_out(fyear,lyear);\n vector F(fyear,lyear);\n \n objective_function_value jnll;\n\n!! log_r = inlog_r;\n!! log_K = inlog_K;\n!! log_q = inlog_q;\n!! log_sd_cpue = inlog_sd_cpue;\n\n\nINITIALIZATION_SECTION\n\n // log_r -0.6\n // log_q -9\n // log_K 8.5 \n log_F 1\n\n\nPROCEDURE_SECTION\n int t;\n dvariable expl;\n dvariable sum_sq;\n \n // Convert from log to normal space\n r = mfexp(log_r);\n q = mfexp(log_q);\n K = mfexp(log_K);\n F = mfexp(log_F);\n sd_cat = 0.05;\n sd_cpue = mfexp(log_sd_cpue);\n\n // Project the model forward\n bio(fyear) = K;\n for (t=fyear; t&lt;=lyear; t++) {\n   expl = 1.0/(1.0+F(t));\n   bio(t+1) = bio(t) + r*bio(t)*(1.0-bio(t)/K) - expl*bio(t);\n   cat_hat(t) = expl * bio(t);\n   expl_out(t) = expl;\n   cpue_hat(t) = q * bio(t);\n  } \n  \n // Compute the likelihoods  \n jnll = 0;\n for (t=fyear; t&lt;=lyear; t++) {\n  jnll += 0.5 * square(log(cat(t)/cat_hat(t)) / sd_cat) + log(sd_cat);\n  jnll += 0.5 * square(log(cpue(t)/cpue_hat(t)) / sd_cpue) + log(sd_cpue);\n }\n \n\nGLOBALS_SECTION\n  #include &lt;admodel.h&gt;\n  #include &lt;admb2r.cpp&gt;\n\n\nREPORT_SECTION\n open_r_file(\"out.rdat\", 6, -999);\n  wrt_r_complete_vector(\"obs_cat\", cat);\n  wrt_r_complete_vector(\"obs_cpue\", cpue);\n  wrt_r_complete_vector(\"est_bio\", bio);\n  wrt_r_complete_vector(\"est_cat\", cat_hat);\n  wrt_r_complete_vector(\"est_cpue\", cpue_hat);\n  wrt_r_complete_vector(\"est_expl\", expl_out);\n  wrt_r_item(\"jnll\", jnll);\n close_r_file();\n\n\nFINAL_SECTION\n\n  ofstream myout(\"estpars.dat\",ios::app);\n    myout&lt;&lt; inlog_r &lt;&lt; \" \" &lt;&lt; log_r &lt;&lt; \" \" &lt;&lt; inlog_q &lt;&lt; \" \" &lt;&lt; log_q &lt;&lt; \" \" &lt;&lt; inlog_K &lt;&lt; \" \" &lt;&lt; log_K &lt;&lt; \" \" &lt;&lt; inlog_sd_cpue &lt;&lt; \" \" &lt;&lt; log_sd_cpue &lt;&lt; \" \" &lt;&lt; jnll &lt;&lt; endl;\n  myout.close();\n\n  // extract Hessian matrix\n  open_r_file(\"hessian.rdat\");\n    open_r_matrix(\"hessian\");\n      wrt_r_matrix(get_hessian(),1,1);\n    close_r_matrix();\n  close_r_file();\n\nThe next step is to set up a R script to run the jitter test."
  },
  {
    "objectID": "guides/ADMB_likelihood.html#set-up-r",
    "href": "guides/ADMB_likelihood.html#set-up-r",
    "title": "Likelihood profiling in ADMB using R",
    "section": "3 Set up R",
    "text": "3 Set up R\n\n3.1 Add ADMB to your environment\nThe R script should work on all computer environments (Windows, Mac, Linux) as long as ADMB is properly setup. These are the instructions for each system:\n \nLinux Mint (probably most other Linux):\n\nadd this line to the end of .profile in Home directory:\n\n\nexport PATH=~/admb:$PATH\n\n \nMac\n\nadd this line right before “export PATH” at end of .zprofile in Home directory:\n\n\nPATH=\"~/admb:${PATH}\"\n\n \nWindows:\n\nIn System Environment Variables, add C:\\ADMB-13.2\\\\bin to Path\n\n\n\n3.2 R helper functions\nYou will need these helper files to run ADMB through R:\n\nbase_funs.r - this contains three functions to read and compile a ADMB executable and run the ADMB model\n\ncompile_admb()\nread_admb()\nrun_admb()\n\nclean_admb.r - after you are done running your model, this will clean all the additional ADMB files in your directory.\n\n \nYou can also run this in R to download these files directly to your local directory:\n\ndownload.file(\"https://raw.githubusercontent.com/lidach/addtools/main/R/base_funs.r\", destfile = \"base_funs.r\")\ndownload.file(\"https://raw.githubusercontent.com/lidach/addtools/main/R/clean_admb.r\", destfile = \"clean_admb.r\")\n\n \nThese helper functions will be loaded in R using the source() function (make sure these are in the same directory as the .tpl and .dat files):\n\nsource(\"base_funs.r\")\nsource(\"clean_admb.r\")\n\n\n\n3.3 Compile and run ADMB in R\nThe R helper functions (base_funs.r) has a function (compile_admb.r), which will compile ADMB using a R command. The .tpl name in this tutorial is called “surp_prod_jitter”.\n\ntpl_name &lt;- \"surp_prod_jitter\" # name of the .tpl file\n# compile ADMB\ncompile_admb(fn = tpl_name, verbose = TRUE)\n\nWe will include verbose=TRUE, which will print out the compile messages from ADMB in the R console (should be the same as running the ADMB command prompt):\n \n\n \nNext we will create new .dat files, which will contain the starting parameter values that will be read into ADMB.\n\ncat(\"-0.6\", file = \"log_r.dat\", sep = \"\\n\")\ncat(\"-3\", file = \"log_q.dat\", sep = \"\\n\")\ncat(\"8\", file = \"log_K.dat\", sep = \"\\n\")\ncat(\"-1\", file = \"log_sd_cpue.dat\", sep = \"\\n\")\n\nThis will create four .dat files (log_r.dat,log_q_dat, log_K.dat, and log_sd_cpue.dat). You should see this in your local directory:\n \n\n\n\n\n\n \nThe .dat files should contain each initial value that is specified (open each file and check):\n\n-0.6 for log_r\n-3 for log_q\n8 for log_K\n-1 for log_sd_cpue\n\n \nNext, we will run the ADMB model using the command run_admb():\n\nrun_admb(fn = tpl_name, verbose = TRUE)\n\nWhat prints out in the R console should look exactly like what prints out in the ADMB command prompt:\n\nThis should produce a result of the surplus production model (check the “.par” file).\n\n\n3.4 Set up the jitter test\nNext, we will create an object called dat, which will show what is being read into estpars.dat. This dat object is just a check to make sure the file contains the correct values (initial starting values from R, parameter estimates from ADMB, and the objective function from ADMB).\n\nif (file.exists(\"estpars.dat\")) {\n  dat &lt;- read.table(\"estpars.dat\")\n  colnames(dat) &lt;- c(\"inlog_r\", \"log_r\", \"inlog_q\", \"log_q\", \"inlog_K\", \"log_K\", \"inlog_sd_cpue\", \"log_sd_cpue\", \"objn\")\n}\n\n \n\n \nWe will then delete estpars.dat as this contains the initial run of the surplus production model. We will also create a replacement file that will have the same column names as dat object in R (and the same as the one created in the FINAL_SECTION of ADMB). We will use the estpars.dat file to store each iteration of the jitter test:\n\n# Delete any existing version of estpars.dat\nif (file.exists(\"estpars.dat\")) file.remove(\"estpars.dat\")\n\n# Create header for file so we know the variables.\n# sep [\"\\n\" needed for line feed]\ncat(\"inlog_r log_r inlogq log_q inlogK log_K inlog_sd_cpue log_sd_cpue objn\", file = \"estpars.dat\", sep = \"\\n\")\n\nThis is what the estpars.dat file should look like:\n \n\n \nNext we will create a set of starting values. We will declare how many iterations of the jitter test we would like to conduct (nrun &lt;- # number of iterations). We will randomize a set of starting values for \\(r\\), \\(q\\), \\(K\\), and \\(sd_{cpue}\\) using the rnorm() function, and it will be randomized with a CV = 10%:\n\n# Define a set of starting values\nnrun &lt;- 50 # number of reruns with new values\nst_log_r &lt;- dat$log_r + rnorm(nrun, sd = 0.1)\nst_log_q &lt;- dat$log_q + rnorm(nrun, sd = 0.1)\nst_log_K &lt;- dat$log_K + rnorm(nrun, sd = 0.1)\nst_log_sd_cpue &lt;- dat$log_sd_cpue + rnorm(nrun, sd = 0.1)\n\nYou should get different initial parameter values for \\(r\\), \\(q\\), \\(K\\), and \\(sd_{cpue}\\) (this is what st_log_r looks like):\n \n\n \nThis is where ADMB will run 50 times in a for loop, with each iteration reading different values of \\(r\\), \\(q\\), \\(K\\), and \\(sd_{cpue}\\) from the objects st_log_r, st_log_q, st_log_K, and st_log_sd_cpue . We will use the system() function to rerun the ADMB model from the executable file (.exe):\n\n# Write out each value of the parameters and run ADMB program for each in loop\nfor (i in 1:length(st_log_r)) {\n  cat(st_log_r[i], file = \"log_r.dat\", sep = \"\") # write one st value to file\n  cat(st_log_q[i], file = \"log_q.dat\", sep = \"\") # write one st value to file\n  cat(st_log_K[i], file = \"log_K.dat\", sep = \"\") # write one st value to file\n  cat(st_log_sd_cpue[i], file = \"log_sd_cpue.dat\", sep = \"\") # write one st value to file\n  if(Sys.info()[\"sysname\"] == \"Windows\") { # windows\n    system(paste0(tpl_name, \".exe\")) \n  } else { # most Mac's and Linux\n    system(paste0(\"./\", tpl_name)) \n  }\n}\n\nThese lines of code will automatically detect the system you are using (Sys.info() command), so this will work on all computer systems. Note that we do not need to manually recompile the model from the ADMB command prompt or use the admb command to rerun the model.\n\n\n3.5 Jitter test results\nNow we will read in the estpars.dat file, which should contain all the iterations of the jitter test with different starting values of each parameter:\n\n# read in and print results to console\njit_res &lt;- read.table(\"estpars.dat\", header = T)\njit_res\n\nWhen you look at the jit_res object, it should look like this:\n \n\n \nThere are some things to look out for in the jit_res object:\n\nThe initial starting parameter values are different across the iterations\nMake sure that the parameter estimates across the iterations are the same (note: there may be some rounding differences, but the estimates should not be significantly different)\nThe objective function (joint negative log likelihood in this tutorial) are the same across the iterations\n\n \nWe can also visualize the jitter test using a box plot:\n\n# boxplots - are there any weird shapes/outliers?\nboxplot(jit_res[, c(2, 4, 6, 8, 9)])\n\n\n\n\n\n\n \nAfter conducting the jitter test, we can run these functions that will clean the extra ADMB files that were compiled throughout the process:\n\n# clean extra files \nclean_admb(fn = tpl_name) \nif (file.exists(\"estpars.dat\")) file.remove(\"estpars.dat\") \nif (file.exists(\"out.rdat\")) file.remove(\"out.rdat\") \nif (file.exists(\"hessian.rdat\")) file.remove(\"hessian.rdat\") \nif (file.exists(\"log_K.dat\")) file.remove(\"log_K.dat\") \nif (file.exists(\"log_r.dat\")) file.remove(\"log_r.dat\") \nif (file.exists(\"log_q.dat\")) file.remove(\"log_q.dat\") \nif (file.exists(\"log_sd_cpue.dat\")) file.remove(\"log_sd_cpue.dat\")\n\n\n\n3.6 Entire R script for the jitter test of the surplus production model\nHere is the entire R script for an example of conducting a jitter test with the surplus production model.\n\nsource(\"base_funs.r\")\nsource(\"clean_admb.r\")\ntpl_name &lt;- \"surp_prod_jitter\"\n\n# compile ADMB\ncompile_admb(fn = tpl_name, verbose = TRUE)\n\n# set initial values and source from external files\ncat(\"-0.6\", file = \"log_r.dat\", sep = \"\\n\")\ncat(\"-3\", file = \"log_q.dat\", sep = \"\\n\")\ncat(\"8\", file = \"log_K.dat\", sep = \"\\n\")\ncat(\"-1\", file = \"log_sd_cpue.dat\", sep = \"\\n\")\n\n# run ADMB\nrun_admb(fn = tpl_name, verbose = TRUE)\n\n# get parameter estimates (used for jittering)\nif (file.exists(\"estpars.dat\")) {\n  dat &lt;- read.table(\"estpars.dat\")\n  colnames(dat) &lt;- c(\"inlog_r\", \"log_r\", \"inlog_q\", \"log_q\", \"inlog_K\", \"log_K\", \"inlog_sd_cpue\", \"log_sd_cpue\", \"objn\")\n}\n\n# Delete any existing version of estpars.dat\nif (file.exists(\"estpars.dat\")) file.remove(\"estpars.dat\")\n\n# Create header for file so we know the variables.\n# sep [\"\\n\" needed for line feed]\ncat(\"inlog_r log_r inlogq log_q inlogK log_K inlog_sd_cpue log_sd_cpue objn\", file = \"estpars.dat\", sep = \"\\n\")\n\n# Define a set of starting values\nnrun &lt;- 50 # number of reruns with new values\nst_log_r &lt;- dat$log_r + rnorm(nrun, sd = 0.1)\nst_log_q &lt;- dat$log_q + rnorm(nrun, sd = 0.1)\nst_log_K &lt;- dat$log_K + rnorm(nrun, sd = 0.1)\nst_log_sd_cpue &lt;- dat$log_sd_cpue + rnorm(nrun, sd = 0.1)\n\n# Write out each value of the parameters and run ADMB program for each in loop\nfor (i in 1:length(st_log_r)) {\n  cat(st_log_r[i], file = \"log_r.dat\", sep = \"\") # write one st value to file\n  cat(st_log_q[i], file = \"log_q.dat\", sep = \"\") # write one st value to file\n  cat(st_log_K[i], file = \"log_K.dat\", sep = \"\") # write one st value to file\n  cat(st_log_sd_cpue[i], file = \"log_sd_cpue.dat\", sep = \"\") # write one st value to file\n  if(Sys.info()[\"sysname\"] == \"Windows\") { # windows\n    system(paste0(tpl_name, \".exe\")) \n  } else { # most Mac's and Linux\n    system(paste0(\"./\", tpl_name)) \n  }\n}\n\n# read in and print results to console\njit_res &lt;- read.table(\"estpars.dat\", header = T)\njit_res\n\n# boxplots - are there any weird shapes/outliers?\nboxplot(jit_res[, c(2, 4, 6, 8, 9)])\n\n# clean extra files\nclean_admb(fn = tpl_name)\nif (file.exists(\"estpars.dat\")) file.remove(\"estpars.dat\")\nif (file.exists(\"out.rdat\")) file.remove(\"out.rdat\")\nif (file.exists(\"hessian.rdat\")) file.remove(\"hessian.rdat\")\nif (file.exists(\"log_K.dat\")) file.remove(\"log_K.dat\")\nif (file.exists(\"log_r.dat\")) file.remove(\"log_r.dat\")\nif (file.exists(\"log_q.dat\")) file.remove(\"log_q.dat\")\nif (file.exists(\"log_sd_cpue.dat\")) file.remove(\"log_sd_cpue.dat\")"
  },
  {
    "objectID": "guides/ADMB_likelihood.html#jitter-test-interpretation",
    "href": "guides/ADMB_likelihood.html#jitter-test-interpretation",
    "title": "Likelihood profiling in ADMB using R",
    "section": "4 Jitter test interpretation",
    "text": "4 Jitter test interpretation\nThere should not be any outliers or weird shapes in the box plots (this is an example of how the box plot would look like if the jitter test fails):\n \n\n\n\n\n\nA failed jitter test is indicative that there is something wrong with the model:\n\nIncorrect parameterization\nBad starting values (they may not make sense for the parameter)\nIncorrect specification of the objective function\nIncorrect equations\nData is not informative enough to estimate the parameter\n\n \nThis should be done for every parameter. In this tutorial, the jitter test was not conducted on \\(F\\). However, to fully check the convergence of this model, the jitter test should also be conducted for \\(F\\)."
  },
  {
    "objectID": "guides/ADMB_jitter.html",
    "href": "guides/ADMB_jitter.html",
    "title": "Jitter test in ADMB using R",
    "section": "",
    "text": "A jitter test should be conducted as one of the checks to make sure your model is consistent with convergence. The goal of a jitter test is to check if alternative parameter starting points result in the same final parameter estimates and objective function.\nThis tutorial will walk through how to conduct a jitter test in ADMB. The fastest way to run a jitter test in ADMB is to run it through the R environment, which will create an executable and run ADMB with the executable so you do not need to manually change the initial parameter values each iteration. However, this will require some extra steps to set up your code in both ADMB and R.",
    "crumbs": [
      "Guides",
      "ADMB",
      "Jitter test"
    ]
  },
  {
    "objectID": "guides/ADMB_jitter.html#admb2r.cpp-instructions",
    "href": "guides/ADMB_jitter.html#admb2r.cpp-instructions",
    "title": "Jitter test in ADMB using R",
    "section": "1 admb2r.cpp instructions",
    "text": "1 admb2r.cpp instructions\nThis tutorial requires this package called “admb2r”. This is a collection of AD Model Builder routines for saving complex data structures into a file that can be read into R. This cannot be automatically downloaded in the newer ADMB versions. You can keep a “admb2r.cpp” file where your .tpl and .dat files are, however that requires copying and pasting it every time you want to run a ADMB model.\n \nThese are instructions for using admb2r.cpp permanently. Copy admb2r.cpp to both the following folders:\n\nadmb/include\nadmb/include/contrib\n\n \nA copy of admb2r.cpp is here. Once this is done you do not need to add “admb2r.cpp” to every project folder. This has been tested on Linux Mint, Windows, and Mac.",
    "crumbs": [
      "Guides",
      "ADMB",
      "Jitter test"
    ]
  },
  {
    "objectID": "guides/ADMB_jitter.html#set-up-admb",
    "href": "guides/ADMB_jitter.html#set-up-admb",
    "title": "Jitter test in ADMB using R",
    "section": "2 Set up ADMB",
    "text": "2 Set up ADMB\nIf possible, you should download the most recent ADMB version here. This recent version will print out additional error messages and allow you to run additional functions (like get_hessian()).\n\n2.1 Surplus production model - .tpl and .dat files\nIn this tutorial, we will look at a surplus production model. The .dat file name is called “surp_prod1.dat”. You can name the .tpl file anything, but in this tutorial, it will be called “surp_prod_jitter.tpl”.\n\nDATA_SECTION\n\n!! ad_comm::change_datafile_name(\"surp_prod1.dat\");\n init_int fyear;\n init_int lyear;\n init_vector cat(fyear,lyear);\n init_vector cpue(fyear,lyear);\n\n\nPARAMETER_SECTION\n\n init_number log_r;\n init_number log_q;\n init_number log_K;\n init_number log_sd_cpue;\n init_vector log_F(fyear,lyear);\n \n number r;\n number q;\n number K;\n number sd_cat;\n number sd_cpue;\n\n vector bio(fyear,lyear+1);\n vector cat_hat(fyear,lyear);\n vector cpue_hat(fyear,lyear);\n vector expl_out(fyear,lyear);\n vector F(fyear,lyear);\n \n objective_function_value jnll;\n\n\nINITIALIZATION_SECTION\n\n log_r -0.6\n log_q -1\n log_K 8.5\n log_sd_cpue -1\n log_F 1\n\n\nPROCEDURE_SECTION\n\n int t;\n dvariable expl;\n \n // Convert from log to normal space\n r = mfexp(log_r);\n q = mfexp(log_q);\n K = mfexp(log_K);\n F = mfexp(log_F);\n sd_cat = 0.05;\n sd_cpue = mfexp(log_sd_cpue);\n\n // Project the model forward\n bio(fyear) = K;\n for (t=fyear; t&lt;=lyear; t++) {\n   expl = 1.0/(1.0+F(t));\n   bio(t+1) = bio(t) + r*bio(t)*(1.0-bio(t)/K) - expl*bio(t);\n   cat_hat(t) = expl * bio(t);\n   expl_out(t) = expl;\n   cpue_hat(t) = q * bio(t);\n  } \n  \n // Compute the likelihoods  \n jnll = 0;\n for (t=fyear; t&lt;=lyear; t++) {\n  jnll += 0.5 * square(log(cat(t)/cat_hat(t)) / sd_cat) + log(sd_cat);\n  jnll += 0.5 * square(log(cpue(t)/cpue_hat(t)) / sd_cpue) + log(sd_cpue);\n }\n\n \nGLOBALS_SECTION\n\n  #include &lt;admodel.h&gt;\n  #include &lt;admb2r.cpp&gt;\n\n\nREPORT_SECTION\n open_r_file(\"out.rdat\", 6, -999);\n  wrt_r_complete_vector(\"obs_cat\", cat);\n  wrt_r_complete_vector(\"obs_cpue\", cpue);\n  wrt_r_complete_vector(\"est_bio\", bio);\n  wrt_r_complete_vector(\"est_cat\", cat_hat);\n  wrt_r_complete_vector(\"est_cpue\", cpue_hat);\n  wrt_r_complete_vector(\"est_expl\", expl_out);\n  wrt_r_item(\"jnll\", jnll);\n close_r_file();\n\n\nFINAL_SECTION\n\n  // extract Hessian matrix\n  open_r_file(\"hessian.rdat\");\n    open_r_matrix(\"hessian\");\n      wrt_r_matrix(get_hessian(),1,1);\n    close_r_matrix();\n  close_r_file();\n\nThis tutorial will not go through the surplus production model in details, but the leading parameters are \\(r\\), \\(K\\), \\(q\\), \\(F\\), and \\(sd_{cpue}\\). Note that there is a standard deviation for the catch observations, but that is fixed in this model (\\(sd_{catch}\\) = 0.05) The objective function (jnll) is the total objective function, which is the sum of the data likelihood for catch and index (CPUE) data, both of which follow a log normal distribution.\n \nThe default data file name is different than the .tpl file name. This is done using this command:\n\n!! ad_com::change_datafile_name(\"surp_prod1.dat\");\n\nThe data will be read from “surp_prod1.dat”. This is useful when you have many variants of the model that use the same data. This trick will be useful for the jitter test.\n \nYou can copy and paste the data “surp_prod1.dat” here:\n\n# first year\n1965\n\n# last year\n1988\n\n# Catch\n93.51\n212.444\n195.032\n382.712\n320.43\n402.467\n365.557\n606.084\n377.642\n318.836\n309.374\n389.02\n276.901\n254.251\n170.006\n97.181\n90.523\n176.532\n216.181\n228.672\n212.177\n231.179\n136.942\n212\n\n# Index\n1.78\n1.31\n0.91\n0.96\n0.88\n0.9\n0.87\n0.72\n0.57\n0.45\n0.42\n0.42\n0.49\n0.43\n0.4\n0.45\n0.55\n0.53\n0.58\n0.64\n0.66\n0.65\n0.61\n0.63\n\n\n\n2.2 DATA_SECTION\nIn the surplus production example, we will jitter the following parameters: \\(r\\), \\(K\\), \\(q\\), and \\(sd_{cpue}\\).\n\n!! ad_comm::change_datafile_name(\"log_r.dat\");\n  init_number inlog_r;\n\n!! ad_comm::change_datafile_name(\"log_K.dat\");\n  init_number inlog_K;\n\n!! ad_comm::change_datafile_name(\"log_q.dat\");\n  init_number inlog_q;\n  \n!! ad_comm::change_datafile_name(\"log_sd_cpue.dat\");\n  init_number inlog_sd_cpue;\n\nThis is similar to the previous command, but here the !! ad_com::change_datafile_name(\"log_r.dat\") command is reading parameter values (inlog_r, inlog_q, inlog_K, inlog_sd_cpue) from separate .dat files. This will be important in the R script as you can rewrite this .dat file to produce different starting values that will be read in ADMB one at a time. Note that the parameter name that is being read in (e.g., inlog_r) should be different than the one in the PARAMETER_SECTION (the ones being used in the estimation; e.g., log_r).\n \nThe entire DATA_SECTION will look like this:\n\nDATA_SECTION\n!! ad_comm::change_datafile_name(\"log_r.dat\");\n  init_number inlog_r;\n\n!! ad_comm::change_datafile_name(\"log_K.dat\");\n  init_number inlog_K;\n\n!! ad_comm::change_datafile_name(\"log_q.dat\");\n  init_number inlog_q;\n  \n!! ad_comm::change_datafile_name(\"log_sd_cpue.dat\");\n  init_number inlog_sd_cpue;\n\n!! ad_comm::change_datafile_name(\"surp_prod1.dat\");\n init_int fyear;\n init_int lyear;\n init_vector cat(fyear,lyear);\n init_vector cpue(fyear,lyear);\n\n\n\n2.3 PARAMETER_SECTION\nIn this section, we will be overriding the starting parameter values declared in the INITIALIZATION_SECTION. This is why the parameter names above (e.g., inlog_r) needs to be different than the ones (e.g., log_r) being declared.\n\n!! log_r = inlog_r;\n!! log_K = inlog_K;\n!! log_q = inlog_q;\n!! log_sd_cpue = inlog_sd_cpue;\n\nThis will now read what was stored in the respective .dat files as the starting value of the parameter.\n \nThe entire PARAMETER_SECTION will look like this:\n\nPARAMETER_SECTION\n\n init_number log_r;\n init_number log_q;\n init_number log_K;\n init_number log_sd_cpue;\n init_vector log_F(fyear,lyear);\n \n number r;\n number q;\n number K;\n number sd_cat;\n number sd_cpue;\n\n vector bio(fyear,lyear+1);\n vector cat_hat(fyear,lyear);\n vector cpue_hat(fyear,lyear);\n vector expl_out(fyear,lyear);\n vector F(fyear,lyear);\n \n objective_function_value jnll;\n \n // override starting values and read from .dat files\n!! log_r = inlog_r;\n!! log_K = inlog_K;\n!! log_q = inlog_q;\n!! log_sd_cpue = inlog_sd_cpue;\n\n\n\n2.4 INITIALIZATION_SECTION\nThis section should either be commented out or empty as the .dat file for each parameters will override the starting value for that parameter. This is important for the jitter test to work. It will look like this for the surplus production model:\n\nINITIALIZATION_SECTION\n\n // log_r -0.6\n // log_q -9\n // log_K 8.5\n // log_sd_cpue -1\n log_F 1\n\nThe next sections (PROCEDURE_SECTION, GLOBALS_SECTION, and REPORT_SECTION) should be the same as the original model.\n\n\n2.5 FINAL_SECTION\nIn the final section, you will need to add a command to define an output file stream, write the output, and close the output file. This is using the functionality of “admb2r”.\n\n  ofstream myout(\"estpars.dat\",ios::app);\n    myout&lt;&lt; inlog_r &lt;&lt; \" \" &lt;&lt; log_r &lt;&lt; \" \" &lt;&lt; inlog_q &lt;&lt; \" \" &lt;&lt; log_q &lt;&lt; \" \" &lt;&lt; inlog_K &lt;&lt; \" \" &lt;&lt; log_K &lt;&lt; \" \" &lt;&lt; inlog_sd_cpue &lt;&lt; \" \" &lt;&lt; log_sd_cpue &lt;&lt; \" \" &lt;&lt; jnll &lt;&lt; endl;\n  myout.close();\n\nThe “estpars.dat” file will eventually contain all the input starting values from R, estimated parameters from ADMB, and the objective function from ADMB. This is important as it will contain all the iterations and results of the jitter test and will be read into R as a table.\n \nThe entire FINAL_SECTION will look like this:\n\nFINAL_SECTION\n\n  ofstream myout(\"estpars.dat\",ios::app);\n    myout&lt;&lt; inlog_r &lt;&lt; \" \" &lt;&lt; log_r &lt;&lt; \" \" &lt;&lt; inlog_q &lt;&lt; \" \" &lt;&lt; log_q &lt;&lt; \" \" &lt;&lt; inlog_K &lt;&lt; \" \" &lt;&lt; log_K &lt;&lt; \" \" &lt;&lt; inlog_sd_cpue &lt;&lt; \" \" &lt;&lt; log_sd_cpue &lt;&lt; \" \" &lt;&lt; jnll &lt;&lt; endl;\n  myout.close();\n\n  // extract Hessian matrix\n  open_r_file(\"hessian.rdat\");\n    open_r_matrix(\"hessian\");\n      wrt_r_matrix(get_hessian(),1,1);\n    close_r_matrix();\n  close_r_file();\n\n\n\n2.6 Entire ADMB example of the surplus production model\nHere is the entire ADMB script for an example of conducting a jitter test with the surplus production model:\n\nDATA_SECTION\n\n!! ad_comm::change_datafile_name(\"log_r.dat\");\n  init_number inlog_r;\n\n!! ad_comm::change_datafile_name(\"log_K.dat\");\n  init_number inlog_K;\n\n!! ad_comm::change_datafile_name(\"log_q.dat\");\n  init_number inlog_q;\n  \n!! ad_comm::change_datafile_name(\"log_sd_cpue.dat\");\n  init_number inlog_sd_cpue;\n\n!! ad_comm::change_datafile_name(\"surp_prod1.dat\");\n init_int fyear;\n init_int lyear;\n init_vector cat(fyear,lyear);\n init_vector cpue(fyear,lyear);\n\n\nPARAMETER_SECTION\n\n init_number log_r;\n init_number log_q;\n init_number log_K;\n init_number log_sd_cpue;\n init_vector log_F(fyear,lyear);\n \n number r;\n number q;\n number K;\n number sd_cat;\n number sd_cpue;\n\n vector bio(fyear,lyear+1);\n vector cat_hat(fyear,lyear);\n vector cpue_hat(fyear,lyear);\n vector expl_out(fyear,lyear);\n vector F(fyear,lyear);\n \n objective_function_value jnll;\n\n!! log_r = inlog_r;\n!! log_K = inlog_K;\n!! log_q = inlog_q;\n!! log_sd_cpue = inlog_sd_cpue;\n\n\nINITIALIZATION_SECTION\n\n // log_r -0.6\n // log_q -9\n // log_K 8.5 \n log_F 1\n\n\nPROCEDURE_SECTION\n int t;\n dvariable expl;\n dvariable sum_sq;\n \n // Convert from log to normal space\n r = mfexp(log_r);\n q = mfexp(log_q);\n K = mfexp(log_K);\n F = mfexp(log_F);\n sd_cat = 0.05;\n sd_cpue = mfexp(log_sd_cpue);\n\n // Project the model forward\n bio(fyear) = K;\n for (t=fyear; t&lt;=lyear; t++) {\n   expl = 1.0/(1.0+F(t));\n   bio(t+1) = bio(t) + r*bio(t)*(1.0-bio(t)/K) - expl*bio(t);\n   cat_hat(t) = expl * bio(t);\n   expl_out(t) = expl;\n   cpue_hat(t) = q * bio(t);\n  } \n  \n // Compute the likelihoods  \n jnll = 0;\n for (t=fyear; t&lt;=lyear; t++) {\n  jnll += 0.5 * square(log(cat(t)/cat_hat(t)) / sd_cat) + log(sd_cat);\n  jnll += 0.5 * square(log(cpue(t)/cpue_hat(t)) / sd_cpue) + log(sd_cpue);\n }\n \n\nGLOBALS_SECTION\n  #include &lt;admodel.h&gt;\n  #include &lt;admb2r.cpp&gt;\n\n\nREPORT_SECTION\n open_r_file(\"out.rdat\", 6, -999);\n  wrt_r_complete_vector(\"obs_cat\", cat);\n  wrt_r_complete_vector(\"obs_cpue\", cpue);\n  wrt_r_complete_vector(\"est_bio\", bio);\n  wrt_r_complete_vector(\"est_cat\", cat_hat);\n  wrt_r_complete_vector(\"est_cpue\", cpue_hat);\n  wrt_r_complete_vector(\"est_expl\", expl_out);\n  wrt_r_item(\"jnll\", jnll);\n close_r_file();\n\n\nFINAL_SECTION\n\n  ofstream myout(\"estpars.dat\",ios::app);\n    myout&lt;&lt; inlog_r &lt;&lt; \" \" &lt;&lt; log_r &lt;&lt; \" \" &lt;&lt; inlog_q &lt;&lt; \" \" &lt;&lt; log_q &lt;&lt; \" \" &lt;&lt; inlog_K &lt;&lt; \" \" &lt;&lt; log_K &lt;&lt; \" \" &lt;&lt; inlog_sd_cpue &lt;&lt; \" \" &lt;&lt; log_sd_cpue &lt;&lt; \" \" &lt;&lt; jnll &lt;&lt; endl;\n  myout.close();\n\n  // extract Hessian matrix\n  open_r_file(\"hessian.rdat\");\n    open_r_matrix(\"hessian\");\n      wrt_r_matrix(get_hessian(),1,1);\n    close_r_matrix();\n  close_r_file();\n\nThe next step is to set up a R script to run the jitter test.",
    "crumbs": [
      "Guides",
      "ADMB",
      "Jitter test"
    ]
  },
  {
    "objectID": "guides/ADMB_jitter.html#set-up-r",
    "href": "guides/ADMB_jitter.html#set-up-r",
    "title": "Jitter test in ADMB using R",
    "section": "3 Set up R",
    "text": "3 Set up R\n\n3.1 Add ADMB to your environment\nThe R script should work on all computer environments (Windows, Mac, Linux) as long as ADMB is properly setup. These are the instructions for each system:\n \nLinux Mint (probably most other Linux):\n\nadd this line to the end of .profile in Home directory:\n\n\nexport PATH=~/admb:$PATH\n\n \nMac\n\nadd this line right before “export PATH” at end of .zprofile in Home directory:\n\n\nPATH=\"~/admb:${PATH}\"\n\n \nWindows:\n\nIn System Environment Variables, add C:\\ADMB-13.2\\\\bin to Path\n\n\n\n3.2 R helper functions\nYou will need these helper files to run ADMB through R:\n\nbase_funs.r - this contains three functions to read and compile a ADMB executable and run the ADMB model\n\ncompile_admb()\nread_admb()\nrun_admb()\n\nclean_admb.r - after you are done running your model, this will clean all the additional ADMB files in your directory.\n\n \nYou can also run this in R to download these files directly to your local directory:\n\ndownload.file(\"https://raw.githubusercontent.com/lidach/addtools/main/R/base_funs.r\", destfile = \"base_funs.r\")\ndownload.file(\"https://raw.githubusercontent.com/lidach/addtools/main/R/clean_admb.r\", destfile = \"clean_admb.r\")\n\n \nThese helper functions will be loaded in R using the source() function (make sure these are in the same directory as the .tpl and .dat files):\n\nsource(\"base_funs.r\")\nsource(\"clean_admb.r\")\n\n\n\n3.3 Compile and run ADMB in R\nThe R helper functions (“base_funs.r”) has a function “compile_admb.r” which will compile ADMB using a R command and through the R environment. The .tpl name in this tutorial is called “surp_prod_jitter”.\n\ntpl_name &lt;- \"surp_prod_jitter\" # name of the .tpl file\n# compile ADMB\ncompile_admb(fn = tpl_name, verbose = TRUE)\n\nWe will include verbose = TRUE, which will print out the compile messages from ADMB in the R console (should be the same as running the ADMB command prompt):\n \n\n \nNext we will create new .dat files, which will contain the starting parameter values that will be read into ADMB.\n\ncat(\"-0.6\", file = \"log_r.dat\", sep = \"\\n\")\ncat(\"-3\", file = \"log_q.dat\", sep = \"\\n\")\ncat(\"8\", file = \"log_K.dat\", sep = \"\\n\")\ncat(\"-1\", file = \"log_sd_cpue.dat\", sep = \"\\n\")\n\nThis will create four .dat files (“log_r.dat”,“log_q_dat”, “log_K.dat”, and “log_sd_cpue.dat”). You should see this in your local directory:\n \n\n\n\n\n\n \nThe .dat files should contain each initial value that is specified (open each file and check):\n\n-0.6 for log_r\n-3 for log_q\n8 for log_K\n-1 for log_sd_cpue\n\n \nNext, we will run the ADMB model using the command run_admb():\n\nrun_admb(fn = tpl_name, verbose = TRUE)\n\nWhat prints out in the R console should look exactly like what prints out in the ADMB command prompt:\n \n\n \nThis should produce a model result of the surplus production model (check the “.par” file).\n\n\n3.4 Set up the jitter test\nNext, we will create an object called dat, which will show what is being read into “estpars.dat”. This dat object is just a check to make sure the file contains the correct values (initial starting values from R, parameter estimates from ADMB, and the objective function from ADMB).\n\nif (file.exists(\"estpars.dat\")) {\n  dat &lt;- read.table(\"estpars.dat\")\n  colnames(dat) &lt;- c(\"inlog_r\", \"log_r\", \"inlog_q\", \"log_q\", \"inlog_K\", \"log_K\", \"inlog_sd_cpue\", \"log_sd_cpue\", \"objn\")\n}\n\n \n\n \nWe will then delete “estpars.dat” as this contains the first run of the surplus production model. We will also create a replacement file (same name - “estpars.dat”) that will have the same column names as dat object in R (and the same as the ones created in the FINAL_SECTION of ADMB). We will use the “estpars.dat” file to store each iteration of the jitter test:\n\n# Delete any existing version of estpars.dat\nif (file.exists(\"estpars.dat\")) file.remove(\"estpars.dat\")\n\n# Create header for file so we know the variables.\n# sep [\"\\n\" needed for line feed]\ncat(\"inlog_r log_r inlogq log_q inlogK log_K inlog_sd_cpue log_sd_cpue objn\", file = \"estpars.dat\", sep = \"\\n\")\n\nThis is what the “estpars.dat” file should look like:\n \n\n \nNext we will create a set of starting values. We will declare how many iterations of the jitter test we would like to conduct (nrun &lt;- # number of iterations). We will randomize a set of starting values for \\(r\\), \\(q\\), \\(K\\), and \\(sd_{cpue}\\) using the rnorm() function, and it will be randomized with a CV = 10% (you can test different CV/sd values, but make sure the value still make sense for the parameters):\n\n# Define a set of starting values\nnrun &lt;- 50 # number of reruns with new values\nst_log_r &lt;- dat$log_r + rnorm(nrun, sd = 0.1)\nst_log_q &lt;- dat$log_q + rnorm(nrun, sd = 0.1)\nst_log_K &lt;- dat$log_K + rnorm(nrun, sd = 0.1)\nst_log_sd_cpue &lt;- dat$log_sd_cpue + rnorm(nrun, sd = 0.1)\n\nYou should get different initial parameter values for \\(r\\), \\(q\\), \\(K\\), and \\(sd_{cpue}\\) (this is what st_log_r looks like):\n \n\n \nThis is where ADMB will run 50 times in a for loop, with each iteration reading different values of \\(r\\), \\(q\\), \\(K\\), and \\(sd_{cpue}\\) from the objects st_log_r, st_log_q, st_log_K, and st_log_sd_cpue . We will use the system() function to rerun the ADMB model from the executable file (.exe):\n\n# Write out each value of the parameters and run ADMB program for each in loop\nfor (i in 1:length(st_log_r)) {\n  cat(st_log_r[i], file = \"log_r.dat\", sep = \"\") # write one st value to file\n  cat(st_log_q[i], file = \"log_q.dat\", sep = \"\") # write one st value to file\n  cat(st_log_K[i], file = \"log_K.dat\", sep = \"\") # write one st value to file\n  cat(st_log_sd_cpue[i], file = \"log_sd_cpue.dat\", sep = \"\") # write one st value to file\n  if(Sys.info()[\"sysname\"] == \"Windows\") { # windows\n    system(paste0(tpl_name, \".exe\")) \n  } else { # most Mac's and Linux\n    system(paste0(\"./\", tpl_name)) \n  }\n}\n\nThese lines of code will automatically detect the system you are using (Sys.info() command), so this will work on all computer systems. Note that we do not need to manually recompile the model from the ADMB command prompt or use the admb command to rerun the model.\n\n\n3.5 Jitter test results\nNow we will read in the “estpars.dat” file, which should contain all the iterations of the jitter test with different starting values of each parameter:\n\n# read in and print results to console\njit_res &lt;- read.table(\"estpars.dat\", header = T)\njit_res\n\nWhen you look at the jit_res object, it should look like this:\n \n\n \nThere are some things to look out for in the jit_res object:\n\nThe initial starting parameter values are different across the 50 iterations (st_log_r, st_log_q, st_log_K, and st_log_sd_cpue)\nMake sure that the parameter estimates across the 50 iterations are the same (log_r, log_q, log_K, and log_sd_cpue) (note: there may be some rounding differences, but the estimates should not be significantly different)\nThe objective function (joint negative log likelihood in this tutorial) are the same across the 50 iterations (objn).\n\n \nWe can also visualize the jitter test using a box plot. The box plot will show if there are any odd shapes in the box plot or any outliers. A jitter test that passes a convergence check will show no odd shapes in the box plot (no quartiles) and will not have any outliers (as shown below):\n\n# boxplots - are there any weird shapes/outliers?\nboxplot(jit_res[, c(2, 4, 6, 8, 9)])\n\n\n\n\n\n\n \nAfter conducting the jitter test, we can run these functions that will clean the extra ADMB files that were compiled throughout the process:\n\n# clean extra files \nclean_admb(fn = tpl_name) \nif (file.exists(\"estpars.dat\")) file.remove(\"estpars.dat\") \nif (file.exists(\"out.rdat\")) file.remove(\"out.rdat\") \nif (file.exists(\"hessian.rdat\")) file.remove(\"hessian.rdat\") \nif (file.exists(\"log_K.dat\")) file.remove(\"log_K.dat\") \nif (file.exists(\"log_r.dat\")) file.remove(\"log_r.dat\") \nif (file.exists(\"log_q.dat\")) file.remove(\"log_q.dat\") \nif (file.exists(\"log_sd_cpue.dat\")) file.remove(\"log_sd_cpue.dat\")\n\n\n\n3.6 Entire R script for the jitter test of the surplus production model\nHere is the entire R script for an example of conducting a jitter test with the surplus production model.\n\nsource(\"base_funs.r\")\nsource(\"clean_admb.r\")\ntpl_name &lt;- \"surp_prod_jitter\"\n\n# compile ADMB\ncompile_admb(fn = tpl_name, verbose = TRUE)\n\n# set initial values and source from external files\ncat(\"-0.6\", file = \"log_r.dat\", sep = \"\\n\")\ncat(\"-3\", file = \"log_q.dat\", sep = \"\\n\")\ncat(\"8\", file = \"log_K.dat\", sep = \"\\n\")\ncat(\"-1\", file = \"log_sd_cpue.dat\", sep = \"\\n\")\n\n# run ADMB\nrun_admb(fn = tpl_name, verbose = TRUE)\n\n# get parameter estimates (used for jittering)\nif (file.exists(\"estpars.dat\")) {\n  dat &lt;- read.table(\"estpars.dat\")\n  colnames(dat) &lt;- c(\"inlog_r\", \"log_r\", \"inlog_q\", \"log_q\", \"inlog_K\", \"log_K\", \"inlog_sd_cpue\", \"log_sd_cpue\", \"objn\")\n}\n\n# Delete any existing version of estpars.dat\nif (file.exists(\"estpars.dat\")) file.remove(\"estpars.dat\")\n\n# Create header for file so we know the variables.\n# sep [\"\\n\" needed for line feed]\ncat(\"inlog_r log_r inlogq log_q inlogK log_K inlog_sd_cpue log_sd_cpue objn\", file = \"estpars.dat\", sep = \"\\n\")\n\n# Define a set of starting values\nnrun &lt;- 50 # number of reruns with new values\nst_log_r &lt;- dat$log_r + rnorm(nrun, sd = 0.1)\nst_log_q &lt;- dat$log_q + rnorm(nrun, sd = 0.1)\nst_log_K &lt;- dat$log_K + rnorm(nrun, sd = 0.1)\nst_log_sd_cpue &lt;- dat$log_sd_cpue + rnorm(nrun, sd = 0.1)\n\n# Write out each value of the parameters and run ADMB program for each in loop\nfor (i in 1:length(st_log_r)) {\n  cat(st_log_r[i], file = \"log_r.dat\", sep = \"\") # write one st value to file\n  cat(st_log_q[i], file = \"log_q.dat\", sep = \"\") # write one st value to file\n  cat(st_log_K[i], file = \"log_K.dat\", sep = \"\") # write one st value to file\n  cat(st_log_sd_cpue[i], file = \"log_sd_cpue.dat\", sep = \"\") # write one st value to file\n  if(Sys.info()[\"sysname\"] == \"Windows\") { # windows\n    system(paste0(tpl_name, \".exe\")) \n  } else { # most Mac's and Linux\n    system(paste0(\"./\", tpl_name)) \n  }\n}\n\n# read in and print results to console\njit_res &lt;- read.table(\"estpars.dat\", header = T)\njit_res\n\n# boxplots - are there any weird shapes/outliers?\nboxplot(jit_res[, c(2, 4, 6, 8, 9)])\n\n# clean extra files\nclean_admb(fn = tpl_name)\nif (file.exists(\"estpars.dat\")) file.remove(\"estpars.dat\")\nif (file.exists(\"out.rdat\")) file.remove(\"out.rdat\")\nif (file.exists(\"hessian.rdat\")) file.remove(\"hessian.rdat\")\nif (file.exists(\"log_K.dat\")) file.remove(\"log_K.dat\")\nif (file.exists(\"log_r.dat\")) file.remove(\"log_r.dat\")\nif (file.exists(\"log_q.dat\")) file.remove(\"log_q.dat\")\nif (file.exists(\"log_sd_cpue.dat\")) file.remove(\"log_sd_cpue.dat\")",
    "crumbs": [
      "Guides",
      "ADMB",
      "Jitter test"
    ]
  },
  {
    "objectID": "guides/ADMB_jitter.html#jitter-test-interpretation",
    "href": "guides/ADMB_jitter.html#jitter-test-interpretation",
    "title": "Jitter test in ADMB using R",
    "section": "4 Jitter test interpretation",
    "text": "4 Jitter test interpretation\nThere should not be any outliers or weird shapes in the box plots (this is an example of how the box plot would look like if the jitter test fails):\n \n\n\n\n\n\nA failed jitter test is indicative that there is something wrong with the model:\n\nIncorrect parameterization\nBad starting values (they may not make sense for the parameter)\nIncorrect specification of the objective function\nIncorrect equations\nData is not informative enough to estimate the parameter\n\n \nThis should be done for every parameter. In this tutorial, the jitter test was not conducted on \\(F\\) and \\(sd_{catch}\\). However, to fully check the convergence of this model, the jitter test should also be conducted for \\(F\\) and sensitivity analysis should be conducted on the impacts of fixing \\(sd_{catch}\\).",
    "crumbs": [
      "Guides",
      "ADMB",
      "Jitter test"
    ]
  },
  {
    "objectID": "guides/diagnostics.html",
    "href": "guides/diagnostics.html",
    "title": "Model convergence diagnostics for non-linear models",
    "section": "",
    "text": "This tutorial discusses what to check to make sure your model is consistent with convergence diagnostics. While this tutorial is focused on TMB/RTMB functions, this checklist can be applied to other non-linear models coded in other programs (e.g., ADMB).",
    "crumbs": [
      "Guides",
      "Model convergence diagnostics"
    ]
  },
  {
    "objectID": "guides/diagnostics.html#checklist",
    "href": "guides/diagnostics.html#checklist",
    "title": "Model convergence diagnostics for non-linear models",
    "section": "1 Checklist:",
    "text": "1 Checklist:\nThis is a checklist of convergence checks and diagnostics that should be conducted at minimum to verify a model:\n Model is executable (i.e., check objective function and gradients) \n The convergence message from RTMB indicates that the diagnostics are consistent with convergence (= 0) \n The Hessian matrix is positive definite \n Standard errors for model estimates are reasonable \n Alternative parameter starting points result in the same final parameter estimates (i.e., jitter test) \n Likelihood profiles for important parameters are reasonable, e.g. (initial abundance, recruitment, natural mortality, selectivity): \n \nIt is recommended to do these checks in this order. If all of these checks pass, this indicates that the model is consistent with convergence and is estimable. It is necessary for the model to pass all these checks. If one of these checks fail, then the model is not consistent with convergence.",
    "crumbs": [
      "Guides",
      "Model convergence diagnostics"
    ]
  },
  {
    "objectID": "guides/diagnostics.html#creating-the-objective-function-in-rtmb",
    "href": "guides/diagnostics.html#creating-the-objective-function-in-rtmb",
    "title": "Model convergence diagnostics for non-linear models",
    "section": "2 Creating the objective function in RTMB",
    "text": "2 Creating the objective function in RTMB\nSay you construct an objective function called obj using f as the model function and par as a list of initial parameter values. Using RTMB::MakeADFun, you can create the objective function:\n\nobj &lt;- RTMB::MakeADFun(f, par)\n\nYou need to check if the objective function will run before running an optimizer. You can look at this by checking if the objective function produces a likelihood and if a gradient is calculated for each parameter:\n\n# check likelihood\nobj$fn()\n\n# check gradients\nobj$gr()\n\n\n\n\n\n\n\nWhat are gradients\n\n\n\n\n\nGradients are the partial derivatives of the objective function with respect to the model parameters (goes into one vector - obj$gr()). These partial derivatives indicate how the objective function changes (direction and magnitude) as each parameter varies. This provides information for optimization algorithms to adjust the parameters iteratively to minimize or maximize the objective function.\n\n\n\nIf the model is estimable, it will calculate a likelihood based on the initial parameters. Each parameter should also provide a gradient value. If the gradient of a parameter = 0 or NA, this means that the model is not able to estimate that parameter or the parameter is not being used in the model.\nIf the checks on the objective function are successful, you can run the objective function with an optimizer using the nlminb function and opt is the output:\n\nopt &lt;- nlminb(obj$par, obj$fn, obj$gr)",
    "crumbs": [
      "Guides",
      "Model convergence diagnostics"
    ]
  },
  {
    "objectID": "guides/diagnostics.html#convergence-message",
    "href": "guides/diagnostics.html#convergence-message",
    "title": "Model convergence diagnostics for non-linear models",
    "section": "3 Convergence message",
    "text": "3 Convergence message\n\n# check if the model is converged\nopt$convergence\n# check type of convergence\nopt$message\n\nIf the diagnostics are consistent with convergence, then opt$convergence = 0. If the model did not converge, opt$convergence = 1. There can be some reasons why the model failed to converge (check opt$message for convergence message):\n\nsingular convergence: model is likely overparameterized (too complex for the data, the data does not contain enough information to estimate the parameters reliably)\nfalse convergence: likelihood may be discontinuous (this could be related to the estimation of the parameters)\n\nIdeally, you want the convergence message: “relative convergence”.\n \nYou may also encounter messages like these:\n\nWarning messages:\n1: In nlminb(start = par, objective = fn, gradient = gr) :\n  NA/NaN function evaluation\n\nThis does not necessarily mean the model is not converged. This means that the optimizer wandered off into a bad region for a while (i.e., NAs/NaNs in the estimates) and may have gotten back out. As long as it is back in a good region by the end of the optimization, then it may be fine. However, this should be evaluated with caution, it can sometimes mean that the parameterization or model equations are not correct. Ideally, you do not want a model that is able to wander off into a bad region.",
    "crumbs": [
      "Guides",
      "Model convergence diagnostics"
    ]
  },
  {
    "objectID": "guides/diagnostics.html#hessian-matrix",
    "href": "guides/diagnostics.html#hessian-matrix",
    "title": "Model convergence diagnostics for non-linear models",
    "section": "4 Hessian matrix",
    "text": "4 Hessian matrix\n\n\n\n\n\n\nWhat is a Hessian matrix?\n\n\n\n\n\nA Hessian matrix is a square matrix of second-order partial derivatives of the objective function. In other words, it contains information about how the rate of change of each parameter with respect to every other parameter changes.\nThis represents the curvature of the likelihood surface and is used to calculate estimates of uncertainty for all the estimated model parameters and chosen derived quantities.\n\nInverting the negative Hessian gives us the covariance matrix, which provides a measure of parameter uncertainty.\nThe diagonal elements of the covariance matrix (i.e., inverse of the Hessian matrix) represent the variance of individual parameters.\nThe square root of the diagonal elements (i.e., variance) gives standard errors of the parameter estimates.\n\n\n\n\nThe Hessian matrix will not be invertible if the negative log likelihood is not a true minimum. This usually occurs when the model is mis-specified, which could either mean that the model has been written incorrectly so the objective function is not differentiable/estimable with respect to all the parameters. Or the estimated parameters are confounded or overparameterized (i.e., too complex for the data). RTMB will warn you about a non-positive definite Hessian matrix.",
    "crumbs": [
      "Guides",
      "Model convergence diagnostics"
    ]
  },
  {
    "objectID": "guides/diagnostics.html#standard-errors",
    "href": "guides/diagnostics.html#standard-errors",
    "title": "Model convergence diagnostics for non-linear models",
    "section": "5 Standard errors",
    "text": "5 Standard errors\nIf the standard errors for the parameter estimates are high, this suggests that the model is not fully converged. This can mean:\n\nlow precision: there is a wide range of plausible values for the parameters\nlack of stability in the estimation of the parameter\noverfitting: model may be too complex and is capturing noise in the data rather than true underlying patterns\n\nConsiderations to the model formulation and parameter estimates should be made if the standard errors are too high and unreasonable.\n\nsdrep &lt;- sdreport(obj)\nsdrep",
    "crumbs": [
      "Guides",
      "Model convergence diagnostics"
    ]
  },
  {
    "objectID": "guides/diagnostics.html#jitter-test",
    "href": "guides/diagnostics.html#jitter-test",
    "title": "Model convergence diagnostics for non-linear models",
    "section": "6 Jitter test",
    "text": "6 Jitter test\nA jitter test is used to evaluate whether a model has actually converged to a global solution rather than a local minimum. This should check that none of the randomly generated starting values of the parameters results in a solution that has a smaller negative log likelihood than the reference model. A jitter test is conducted by changing the starting parameter values and rerunning the model several times. This should be done with multiple (if not all) the parameters.\n\n# this function runs the optimizer with the new parameters (opt$par + randomization)\ndoone &lt;- function() {\n  fit &lt;- nlminb(opt$par + rnorm(length(opt$par), sd = .1),\n    obj$fn, obj$gr,\n    control = list(eval.max = 1000, iter.max = 1000)\n  )\n  c(fit$par, \"convergence\" = fit$convergence)\n}\n\nset.seed(123456)\n# jitter the parameters 100 times\njit &lt;- replicate(100, doone())\n# check if the convergence are all 0s across the 100 iterations\n# check if there are any outliers or large intervals\nboxplot(t(jit))\n\nNote: the magnitude of the jittering should be done within reason (e.g., 10% CV) as extreme jitters could start the model search in an unrealistic place and then the model would not be able to detect gradients that point towards reasonable solutions.",
    "crumbs": [
      "Guides",
      "Model convergence diagnostics"
    ]
  },
  {
    "objectID": "guides/diagnostics.html#likelihood-profile",
    "href": "guides/diagnostics.html#likelihood-profile",
    "title": "Model convergence diagnostics for non-linear models",
    "section": "7 Likelihood profile",
    "text": "7 Likelihood profile\nA likelihood profile shows how the negative log likelihood changes as one of the parameters is fixed while estimating the other parameters. This helps evaluate which parameters are informative, measure the amount of information contained in the data, and check the sensitivity (i.e., the consequence of using a fixed value) of the model result to the choice of the parameters. The shape of the likelihood profile for a parameter should look like a U:\n\n\n\n\n\nThis does not have to be done on all parameters, but select important parameters. This typically includes initial abundance (e.g., R0, initial abundance at age vector), recruitment (e.g., steepness or compensation ratio), natural mortality, selectivity, etc. The goal is to identify if there are any conflicting information in the data about abundance.",
    "crumbs": [
      "Guides",
      "Model convergence diagnostics"
    ]
  },
  {
    "objectID": "guides/naming.html",
    "href": "guides/naming.html",
    "title": "Naming convention - data and parameters",
    "section": "",
    "text": "obs_xx observed data\nn_year\nyears\nn_age\nages\nlinf\nvbk\nto\nla\nwa\nmat\nfec\nM\nobs_ct\nobs_eff\nobs_comp\nneff"
  },
  {
    "objectID": "guides/naming.html#data",
    "href": "guides/naming.html#data",
    "title": "Naming convention - data and parameters",
    "section": "",
    "text": "obs_xx observed data\nn_year\nyears\nn_age\nages\nlinf\nvbk\nto\nla\nwa\nmat\nfec\nM\nobs_ct\nobs_eff\nobs_comp\nneff"
  },
  {
    "objectID": "guides/naming.html#parameters",
    "href": "guides/naming.html#parameters",
    "title": "Naming convention - data and parameters",
    "section": "2 Parameters",
    "text": "2 Parameters\n\nlog_xx log space data/parameters\nr_init\nar\nbr\nsig\nsel_p1\nsel_p2\nq\nsdr\nsdc"
  },
  {
    "objectID": "guides/naming.html#model",
    "href": "guides/naming.html#model",
    "title": "Naming convention - data and parameters",
    "section": "3 Model",
    "text": "3 Model\n\nF\nssb\nct\njnll"
  },
  {
    "objectID": "guides/stock_assessment.html",
    "href": "guides/stock_assessment.html",
    "title": "Stock assessment - how to set up R script",
    "section": "",
    "text": "This is a tutorial of how to set up script for running a stock assessment model. This is the format that all stock assessment scripts written in R/RTMB and from the QFC will use."
  },
  {
    "objectID": "guides/stock_assessment.html#r-script-header",
    "href": "guides/stock_assessment.html#r-script-header",
    "title": "Stock assessment - how to set up R script",
    "section": "1 R script header",
    "text": "1 R script header\nFirst, create comments that has the model name (if it has a specific name) and any information on the stock and location, modeler’s name, and any important notes on the model. Put dates if you are not using version control (i.e., not using GitHub). Delete any old comments that are not relevant to the most up-to-date model. Right after the comments, put library(RTMB) to open the RTMB R package.\n\n# Model name and information on stock and location\n# Stock assessor name(s) (DD-MM-YYYY)\n# any notes on model should be here, delete any old notes\n\nlibrary(RTMB)"
  },
  {
    "objectID": "guides/stock_assessment.html#data-and-parameters",
    "href": "guides/stock_assessment.html#data-and-parameters",
    "title": "Stock assessment - how to set up R script",
    "section": "2 Data and parameters",
    "text": "2 Data and parameters\nSet up the data list. For RTMB, the data needs to go into a single list, which will be called data. For consistency, keep the names of the variables/data inputs the same as listed here. See [] for naming convention.\n\ndat &lt;- load(\"data_file.RData\")    # data for stock assessment model\ndata &lt;- list()                    # all data for model should go into list called \"data\"\ndata$years &lt;- dat$fyear:dat$lyear \ndata$n_year &lt;- length(data$years)\ndata$ages &lt;- dat$fage:dat$lage \ndata$n_age &lt;- length(data$ages)\n### etc\n\nRight after the data, set up a single list for the parameters, which will be called par. Again, see naming convention here: [].\n\npar &lt;- list()\npar$log_rinit &lt;- 11\npar$log_q &lt;- -1\npar$sel_p1 &lt;- log(7)\npar$sel_p2 &lt;- log(5)\n### etc\n\nIf there are any additional functions that are required to run the model, put it after the parameters and before the function for the assessment model."
  },
  {
    "objectID": "guides/stock_assessment.html#creating-objective-function-i.e.-the-stock-assessment-model",
    "href": "guides/stock_assessment.html#creating-objective-function-i.e.-the-stock-assessment-model",
    "title": "Stock assessment - how to set up R script",
    "section": "3 Creating objective function (i.e., the stock assessment model)",
    "text": "3 Creating objective function (i.e., the stock assessment model)\nCreate the function for the objective function (i.e., stock assessment model). Call this function f. The only argument required is par, which is the parameter list. Note that data is not necessary in the function argument.\nThe first line should be the function getAll(data, par), which is a function that makes all the list of elements of data and parameters visible inside the function so that one could write years instead of data$years. The next line contains \"[&lt;-\" &lt;- ADoverload(\"[&lt;-\"), which helps work around limitations in R’s method dispatch system (e.g., R can’t combine numeric and ADvector classes).\n\nf &lt;- function(par) {\n  getAll(data, par)\n  \"[&lt;-\" &lt;- ADoverload(\"[&lt;-\")\n\n...\n}\n\nNext, put the tranformation of parameters (e.g., parameters in log space). Note that the name of the tranformed parameter should be different than the initial parameter in the par list.\n\nf &lt;- function(par) {\n...\n\n  ## Transform parameters\n  rinit &lt;- exp(log_rinit)\n  ### etc\n\n...\n}\n\n[Sections…]\nThe last part of the stock assessment model (the f function) will be the report section and the value that will be returned from running the function. The REPORT() function tells RTMB that we want to report a calculation from the model. The ADREPORT() function tells RTMB that we want uncertainties for this intermediate calculation from the model. Note that using ADREPORT() will slow down the model run. The value within return() should always be the joint negative log likelihood (jnll) in any stock assessment model.\n\nf &lt;- function(par) {\n...\n\n  ## Report and AD report section\n  REPORT(xx1)\n  REPORT(xx2)\n  ADREPORT(xx)\n\n  return(jnll)\n}"
  },
  {
    "objectID": "guides/stock_assessment.html#set-up-the-objective-function",
    "href": "guides/stock_assessment.html#set-up-the-objective-function",
    "title": "Stock assessment - how to set up R script",
    "section": "4 Set up the objective function",
    "text": "4 Set up the objective function\nWe then have to set up the objective function and define the fixed and/or random effects. The objective function (i.e., the assessment model) f and parameters par are processed by RTMB using the call MakeADFun(f, par).\n\nobj &lt;- MakeADFun(f, par)\n\nRandom effects can be included using the argument random = c(\"p1\", \"p2). A component of the parameter list (par) is marked as random if its name is matched by any of the parameters of the vector random.\n\nobj &lt;- MakeADFun(f, par, random = c(\"log_rinit\", \"log_q\"))\n\nIf some parameters are specified as random effects, these will be integrated out of the objective function via the Laplace approximation. In this situation, the functions fn and gr perform an optimization of random effects of each function evaluation, which is referred to as the “inner optimization”.\nFixed parameters can be included in the argument map = list(\"par1\" = as.factor(NA)). A map is a named list of factors with the following properties:\n\nnames(map) is a subset of names(par)\nFor a parameter “p” length(map$p) equals length(par$p)\nParameter entries with NAs in the factor are fixed\n\n\nobj &lt;- MakeADFun(f, par, map = list(\"log_rinit\" = as.factor(NA)))"
  },
  {
    "objectID": "guides/stock_assessment.html#fitting-the-model",
    "href": "guides/stock_assessment.html#fitting-the-model",
    "title": "Stock assessment - how to set up R script",
    "section": "5 Fitting the model",
    "text": "5 Fitting the model\nWe optimize the model using nlminb. This function at minimum needs initial values for the parameters to be optimized (obj$par), the objective function to be minimized (obj$fn), and gradient of the objective function (obj$gr).\n\nopt &lt;- nlminb(obj$par, obj$fn, obj$gr)"
  },
  {
    "objectID": "guides/stock_assessment.html#calculating-model-outputs",
    "href": "guides/stock_assessment.html#calculating-model-outputs",
    "title": "Stock assessment - how to set up R script",
    "section": "6 Calculating model outputs",
    "text": "6 Calculating model outputs\nUncertainties are calculated using sdreport(obj). This should be saved into an object called sdr. The function sdreport() is used to calculate standard deviations of all model parameters, including non linear functions of random effects and parameters/estimates specified through ADREPORT().\n\nsdr &lt;- sdreport(obj)\n\nThe quantities within the objective function (i.e., anything within REPORT()) can be extracted using obj$report(opt$par). The parameters and standard errors can be extracted as separate lists using as.list(). Pass report = TRUE to get ADREPORTed quantities.\n\nres &lt;- obj$report(opt$par) # REPORT estimates\nres &lt;- as.list(sdr, \"Est\") # get parameter estimates\nres &lt;- as.list(sdr, \"Std\") # parameter uncertainties\n# if ADREPORT()\nresl &lt;- as.list(sdr, \"Est\", report = TRUE)      # ADREPORT estimates\nresl_sd &lt;- as.list(sdrep, \"Std\", report = TRUE) # ADREPORT uncertainties\n\n[plots…]\n[diagnostics…]"
  },
  {
    "objectID": "guides/stock_assessment.html#entire-script",
    "href": "guides/stock_assessment.html#entire-script",
    "title": "Stock assessment - how to set up R script",
    "section": "7 Entire script",
    "text": "7 Entire script\n\n# Model name and information on stock and location\n# Stock assessor name(s) (DD-MM-YYYY)\n# any notes on model should be here, delete any old notes\n\nlibrary(RTMB)\n# Data ----\ndat &lt;- load(\"data_file.RData\")    # data for stock assessment model\ndata &lt;- list()                    # all data for model should go into list called \"data\"\ndata$years &lt;- dat$fyear:dat$lyear \ndata$n_year &lt;- length(data$years)\ndata$ages &lt;- dat$fage:dat$lage \ndata$n_age &lt;- length(data$ages)\n### etc\n\n# Parameters ----\npar &lt;- list()\npar$log_rinit &lt;- 11\npar$log_q &lt;- -1\npar$sel_p1 &lt;- log(7)\npar$sel_p2 &lt;- log(5)\n### etc\n\n# Additional functions ----\nadd_function &lt;- function(arguments) {\n  ...\n  return(out)\n}\n\n# Stock assessment model ----\nf &lt;- function(par) {\n  getAll(data, par)\n  \"[&lt;-\" &lt;- ADoverload(\"[&lt;-\")\n\n  ## Transform parameters\n  rinit &lt;- exp(log_rinit)\n  ### etc\n\n  ## Sections\n\n  ## Report and AD report section\n  REPORT(xx1)\n  REPORT(xx2)\n  ADREPORT(xx)\n\n  return(jnll)\n}\n\n# Run model ----\nobj &lt;- MakeADFun(f, par)\nopt &lt;- nlminb(obj$par, obj$fn, obj$gr)\nsdr &lt;- sdreport(obj)\nres &lt;- obj$report(opt$par) # REPORT estimates\nres &lt;- as.list(sdr, \"Est\") # get parameter estimates\nres &lt;- as.list(sdr, \"Std\") # parameter uncertainties\n# if ADREPORT()\nresl &lt;- as.list(sdr, \"Est\", report = TRUE)      # ADREPORT estimates\nresl_sd &lt;- as.list(sdrep, \"Std\", report = TRUE) # ADREPORT uncertainties\n\n# Plots ----\n\n# Diagnostics ----"
  },
  {
    "objectID": "notes/equil_recruit.html",
    "href": "notes/equil_recruit.html",
    "title": "Equilibrium Recruitment",
    "section": "",
    "text": "Fisheries management decisions are often based on abundance relative to reference points. The most common reference point is the population size at which MSY is achieved. The fully-selected fishing mortality corresponding to MSY (\\(F_{MSY}\\)), which is defined as the fishing mortality at which yield is maximized.\nYield refers to the total amount of fish harvested from a fishery within a given period of time, which is typically measured of the productivity or output of the fishery in terms of weight or number of fish caught.\nNote that yield-per-recruit is different than yield. Yield-per-recruit (YPR) is a measure of the average contribution of an individual fish to the total yield of the fishery. It focuses on the productivity of individual fish within the population rather than the total catch.\nThe recruitment as a function of \\(F\\) (\\(R_F\\)) depends on the assumed form of the stock-recruitment relationship. The unfished spawning biomass (\\(S_0\\)) is the expected spawning biomass for one unfished recruit (i.e., spawner biomass per recruit; \\(\\phi_0\\)) times the number of recruits (i.e., the unfished recruitment; \\(R_0\\))"
  },
  {
    "objectID": "notes/equil_recruit.html#stock-recruitment-functions-in-terms-of-alpha-and-beta",
    "href": "notes/equil_recruit.html#stock-recruitment-functions-in-terms-of-alpha-and-beta",
    "title": "Equilibrium Recruitment",
    "section": "1 Stock recruitment functions in terms of alpha and beta",
    "text": "1 Stock recruitment functions in terms of alpha and beta\n\n1.1 Per-recruit calculations\nYield (\\(Y\\)) can be defined as a function of fully-selected fishing mortality:\n\\[\nY_F = {YPR}_F R_F\n\\tag{1}\\]\n\n\\(YPR_F\\) is yield-per-recruit as a function of fishing mortality\n\\(R_F\\) is recruitment as a function of fishing mortality.\n\nYPR is defined as:\n\\[\n{YPR}_F = \\sum_a w_a \\dfrac{s_a F}{Z_a}N_a (1-e^{-Z_a})\n\\tag{2}\\]\n\n\\(w_a\\) is the weight at age\n\\(s_a\\) is the selectivity at age\n\\(Z_a\\) is the total mortality at age\n\n\\[\nZ_a = M + s_a F\n\\tag{3}\\]\n\n\\(N_a\\) is the number at age relative to the number of fish of age 0\n\n\\[\nN_{a}=\\left\\{\\begin{array}{ll}\n1 & \\text { if } a=0 \\\\\nN_{a-1} e^{-Z_{a-1}} & \\text { if } 0&lt;a&lt;x \\\\\n\\dfrac{N_{A-1} e^{-Z_{A-1}}}{1-e^{-Z_{A}}} & \\text { if } a=A\n\\end{array}\\right.\n\\tag{4}\\]\nThe unfished spawning biomass (\\(S_0\\)) is defined as:\n\\[\nS_0 = \\phi_0 R_0\n\\tag{5}\\]\nThe unfished recruitment (\\(R_0\\)) can be obtained by some algebra with eq 5:\n\\[\nR_0 = \\dfrac{S_0}{\\phi_0}\n\\tag{6}\\]\nThis then also means that the spawner per biomass recruit (\\(\\phi_0\\)) can also be obtained:\n\\[\n\\phi_0 = \\dfrac{S_0}{R_0}\n\\tag{7}\\]\nThe same relationship in eq 5 applies for equilibrium spawning biomass:\n\\[\nS_F = \\phi_F R_F\n\\tag{8}\\]\nSpawner biomass per recruit as a function of \\(F\\) is defined as:\n\\[\n\\phi_F = \\sum_a f_a N_a\n\\tag{9}\\]\n\n\\(f_a\\) is fecundity at age\n\n\n\n1.2 Beverton-Holt\nRecruitment as a function of \\(F\\) for Beverton-Holt is defined as:\n\\[\nR_F = \\dfrac{S_F}{\\alpha + \\beta S_F}\n\\tag{10}\\]\nSubstitute eq 8 into eq 10 and solve for \\(R_F\\) to get the stock recruitment relationship in terms of \\(\\phi_F\\): \\[\nR_F = \\dfrac{\\phi_F - \\alpha}{\\beta\\hspace{0.5mm} \\phi_F}\n\\tag{11}\\]\nUnfished recruitment for Beverton-Holt can be defined the same eq 10:\n\\[\nR_0 = \\dfrac{S_0}{\\alpha + \\beta S_0}\n\\tag{12}\\]\nNow we need a definition for \\(\\alpha\\) and \\(\\beta\\), the stock recruitment parameters. These only vary by \\(R_0\\) and steepness (\\(h\\)), which is defined as the expected recruitment at 20% of \\(S_0\\) (hence \\(0.2 S_0\\)):\n\\[\nh R_0 = \\dfrac{0.2 S_0}{\\alpha + 0.2 \\beta S_0}\n\\tag{13}\\]\nWith some extra algebra, we can obtain \\(\\alpha\\) and \\(\\beta\\) in terms of steepness (\\(h\\)), unfished recruitment (\\(R_0\\)), and unfished spawning biomass (\\(S_0\\)):\n\\[\n\\alpha = \\phi_0 \\left(\\dfrac{1-h}{4h}\\right)\n\\tag{14}\\]\n\n\n\n\n\n\nAlgebra on alpha\n\n\n\n\n\n\n\n\n\n\\[\n\\beta = \\dfrac{5h-1}{4h R_0}\n\\tag{15}\\]\n\n\n\n\n\n\nAlgebra on beta\n\n\n\n\n\n\n\n\n\n\n\n1.3 Ricker\nRecruitment as a function of \\(F\\) for Ricker is defined as:\n\\[\nR_F = \\alpha S_F e^{-\\beta S_F}\n\\tag{16}\\]\nSubstitute eq 8 into eq 16 and solve for \\(R_F\\) to get the stock recruitment relationship in terms of \\(\\phi_F\\): \\[\nR_F = \\dfrac{log(\\alpha \\hspace{0.5mm} \\phi_F)}{\\beta \\hspace{0.5mm} \\phi_F}\n\\tag{17}\\]\nUnfished recruitment for Ricker can be defined the same eq 16:\n\\[\nR_0 = \\alpha S_0 e^{-\\beta S_0}\n\\tag{18}\\]\nNow we need a definition for \\(\\alpha\\) and \\(\\beta\\), the stock recruitment parameters. These only vary by \\(R_0\\) and steepness (\\(h\\)). Steepness can be included as:\n\\[\nh R_0 = 0.2 \\alpha S_0 e^{-0.2 \\beta S_0}\n\\tag{19}\\]\nWith some extra algebra, we can obtain \\(\\alpha\\) and \\(\\beta\\) in terms of steepness (\\(h\\)), unfished recruitment (\\(R_0\\)), and unfished spawning biomass (\\(S_0\\)):\n\\[\n\\alpha = \\dfrac{1}{\\phi_0} \\hspace{0.5mm} e^{\\dfrac{5 log(5h)}{4}}\n\\tag{20}\\]\n\\[\n\\beta = \\dfrac{log(5h)}{0.8 S_0}\n\\tag{21}\\]"
  },
  {
    "objectID": "notes/equil_recruit.html#stock-recruitment-functions-in-terms-of-steepness",
    "href": "notes/equil_recruit.html#stock-recruitment-functions-in-terms-of-steepness",
    "title": "Equilibrium Recruitment",
    "section": "2 Stock recruitment functions in terms of steepness",
    "text": "2 Stock recruitment functions in terms of steepness\nWe are going to define steepness (\\(h\\)) and unfished recruitment \\(R0\\) in terms of \\(\\alpha\\) and \\(\\beta\\), which is a reparameterization of the previous section. The per-recruit calculations are the same as the previous section.\n\n2.1 Beverton-Holt\nSteepness for the Beverton-Holt function is defined as:\n\\[\nh = \\dfrac{\\alpha \\hspace{0.5mm} \\phi_0}{4 + \\alpha \\hspace{0.5mm} \\phi_0}\n\\tag{22}\\]\nThe unfished recruitment (\\(R_0\\)) is defined as (similar to eq 11):\n\\[\nR_0 = \\dfrac{\\phi_0 - \\alpha}{\\beta\\hspace{0.5mm} \\phi_0}\n\\tag{23}\\]\nThe Beverton-Holt function in terms of \\(F\\), steepness (\\(h\\)), and unfished recruitment (\\(R_0\\)) becomes:\n\\[\nR_F = \\dfrac{S_F 4h R_0}{S_0 (1-h) + S_F(5h-1)}\n\\tag{24}\\]\n\n\n2.2 Ricker\nSteepness for the Ricker function is defined as:\n\\[\nh = \\dfrac{(\\alpha \\hspace{0.5mm} \\phi_0)^{4/5}}{5}\n\\tag{25}\\]\nThe unfished recruitment (\\(R_0\\)) is defined as (similar to eq 17):\n\\[\nR_0 = \\dfrac{log(\\alpha \\hspace{0.5mm} \\phi_0)}{\\beta \\hspace{0.5mm} \\phi_0}\n\\tag{26}\\]\nThe Ricker function in terms of \\(F\\), steepness (\\(h\\)), and unfished recruitment (\\(R_0\\)) becomes:\n\\[\nR_F = \\dfrac{S_F}{\\phi_0} (5h)^{\\left( \\dfrac{5}{4} \\right)\\left(1- \\dfrac{S_F}{R_0 \\phi_0} \\right)}\n\\tag{27}\\]"
  },
  {
    "objectID": "notes/equil_recruit.html#stock-recruitment-functions-in-terms-of-compensation-ratio",
    "href": "notes/equil_recruit.html#stock-recruitment-functions-in-terms-of-compensation-ratio",
    "title": "Equilibrium Recruitment",
    "section": "3 Stock recruitment functions in terms of compensation ratio",
    "text": "3 Stock recruitment functions in terms of compensation ratio\nAn alternative estimation of recruitment uses compensation ratio (\\(CR\\)), which is defined as the ratio of \\(\\alpha\\) to the slope of the line drawn between the origin and the point on the stock-recruitment surve at which the stock is unfished (Goodyear, 1997; Myers et al., 1999).\n\\[\n\\]\n\n3.1 Beverton-Holt\n\\[\nCR = \\dfrac{4h}{1-h}\n\\]\n\\[\n\\alpha = \\dfrac{CR}{\\phi_{E_0}}\n\\]\n\\[\n\\beta = \\dfrac{\\alpha \\phi_{E_0} - 1}{R_0 \\phi_{E_0}}\n\\]\n\\[\nR_F = \\dfrac{\\alpha \\phi_{E_F} - 1}{\\beta \\phi_{E_F}}\n\\]\n\n\n3.2 Ricker\n\\[\n\\alpha = \\dfrac{CR}{\\phi_{E_0}}\n\\]\n\\[\n\\beta = \\dfrac{log(\\alpha \\phi_{E_0})}{R_0 \\phi_{E_0}}\n\\]\n\\[\nR_F = \\dfrac{log(\\alpha \\phi_{E_F})}{\\beta \\phi_{E_F}}\n\\]"
  }
]